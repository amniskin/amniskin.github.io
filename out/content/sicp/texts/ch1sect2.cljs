(ns content.sicp.texts.ch1sect2 (:require [tailrecursion.hoplon.markdown :as md] [content.sicp.book-data :as data] [tailrecursion.hoplon :refer [form audio input hgroup do! timeout $text base h1 embed h3 body keygen on-append! progress main cite on-page-load object i p nav ruby check-val! a menu blockquote img $comment span track seq?* data u dl select html thead del eventsource append-child fieldset aside figure figcaption sentinel q on! bdi video address caption parse-args dd rp hr tbody table acronym frame applet html-var add-initfn! pre ul dir html-time add-attributes! html-map sup dfn sub mark script big button wbr strong li dt frameset td tr section th optgroup iframe legend em kbd spliced article isindex abbr command <!-- source output basefont route-cell header datalist tfoot s ins footer title is-ie8 h5 canvas param font div option summary samp center small style textarea loop-tpl* strike h4 tt head add-children! ol details col vector?* label rt when-dom h6 link page-load colgroup meter html-meta text-val! bdo --> b code node? noframes replace-children! noscript safe-nth h2 area br]] [tailrecursion.javelin :refer [input? cell cell? destroy-cell! ^{:private true} last-rank ^{:deprecated true} lift lens? set-formula! cell-doseq* ^{:dynamic true, :private true} *tx* deref* set-cell! lens formula? alts! dosync* cell-map formula]]) (:require-macros [tailrecursion.hoplon.markdown :refer [md]] [tailrecursion.hoplon :refer [text with-timeout sexp defelem def-values with-page-load with-dom loop-tpl with-interval with-init!]] [tailrecursion.javelin :refer [with-let mx2 dosync cell= set-cell!= prop-cell cell-doseq defc cell-let-1 defc= macroexpand-all mx cell-let]]))

(sexp {} "defelem content [_ _]" (div {} (data/sect {:title "Procedures and the Processes They Generate"} (md {} "We have now considered the elements of programming: We have used\nprimitive arithmetic operations, we have combined these operations, and\nwe have abstracted these composite operations by defining them as\ncompound procedures. But that is not enough to enable us to say that we\nknow how to program. Our situation is analogous to that of someone who\nhas learned the rules for how the pieces move in chess but knows nothing\nof typical openings, tactics, or strategy. Like the novice chess player,\nwe don't yet know the common patterns of usage in the domain. We lack the\nknowledge of which moves are worth making (which procedures are worth\ndefining). We lack the experience to predict the consequences of making a\nmove (executing a procedure).\n\nThe ability to visualize the consequences of the actions under\nconsideration is crucial to becoming an expert programmer, just as it is\nin any synthetic, creative activity. In becoming an expert photographer,\nfor example, one must learn how to look at a scene and know how dark each\nregion will appear on a print for each possible choice of exposure and\ndevelopment conditions. Only then can one reason backward, planning\nframing, lighting, exposure, and development to obtain the desired\neffects. So it is with programming, where we are planning the course of\naction to be taken by a process and where we control the process by means\nof a program. To become experts, we must learn to visualize the processes\ngenerated by various types of procedures. Only after we have developed\nsuch a skill can we learn to reliably construct programs that exhibit the\ndesired behavior.\n\nA procedure is a pattern for the local evolution of a computational\nprocess. It specifies how each stage of the process is built upon the\nprevious stage. We would like to be able to make statements about the\noverall, or global, behavior of a process whose local evolution has been\nspecified by a procedure. This is very difficult to do in general, but we\ncan at least try to describe some typical patterns of process evolution.\n\nIn this section we will examine some common \"shapes\" for processes\ngenerated by simple procedures. We will also investigate the rates at\nwhich these processes consume the important computational resources of\ntime and space. The procedures we will consider are very simple. Their\nrole is like that played by test patterns in photography: as\noversimplified prototypical patterns, rather than practical examples in\ntheir own right.")) "" (data/subsect {:title "Linear Recursion and Iteration"} (data/fig {:footer "A linear recursive process for computing 6!."}) (md {} "We begin by considering the factorial function, defined by\n\n$$n!=n\\cdot(n-1)\\cdot(n-2)\\cdot\\cdot\\cdot3\\cdot2\\cdot1$$\n\nThere are many ways to compute factorials. One way is to make use of\nthe observation that *n!* is equal to *n* times *(n-1)!* for any\npositive integer *n*:\n\n$$n!=n\\cdot\\left[(n-1)\\cdot(n-2)\\cdot\\cdot\\cdot3\\cdot2\\cdot1\\right]=n\\cdot(n-1)!$$\n\nThus we can compute *n*! by computing *(n-1)*! and multiplying the\nresult by *n*. If we add the stipulation that 1! is equal to 1, this\nobservation translates directly into a procedure:\n\n```clj\n(defn factorial [n]\n  (if (= n 1)\n      1\n      (* n (factorial (- n 1)))))\n```\n\nWe can use the substitution model of section\n[1.1.5](#/sicp/ch/1/sect/1/sub/5/) to watch this procedure in action\ncomputing 6!, as shown in [figure 1.3](#/sicp/figure1-3).\n\nNow let's take a different perspective on computing factorials. We could describe a rule for computing *n*! by specifying that we first multiply 1 by 2, then multiply the result by 3, then by 4, and so on until we reach *n*. More formally, we maintain a running product, together with a counter that counts from 1 up to *n*. We can describe the computation by saying that the counter and the product simultaneously change from one step to the next according to the rule:\n\n$$\\text{product }\\rightarrow\\text{counter }\\cdot\\text{product}$$\n$$\\text{counter }\\rightarrow\\text{ counter }+1$$\n\nand stipulating that *n*! is the value of the product when the counter exceeds *n*.") "" (data/fig {:footer "A linear iterative process for computing 6!"}) "" (md {} "Once again, we can recast our description as a procedure for computing factorials:<<In a real program we would hide the definition of `fact-iter` using what's called a `let` binding, but we'll learn more about that later on.>>\n\n```clj\n(defn factorial [n]\n  (fact-iter 1 1 n))\n(defn fact-iter [product counter max-count]\n  (if (> counter max-count)\n      product\n      (fact-iter (* counter product)\n                 (+ counter 1)\n                 max-count)))\n```\n\nAs before, we can use the substitution model to visualize the process of computing 6!, as shown in [figure 1.4](#/sicp/figure/1-4).\n\nCompare the two processes. From one point of view, they seem hardly different at all. Both compute the sam emathematical function on the same domain, and each reauires a number of steps proportional to *n* to compute *n*!. Indeed, both processes even carry out the same sequence of multiplications, obtaining the same sequence of partial products. On the other hand, when we consider the \"shapes\" of the two processes, we find that they evolve quite differently.\n\nConsider the first process. The substitution model reveals a shape of expansion followed by contraction, indicated by the arrow in [figure 1.3](#/sicp/figure/1-3). The expansion occurs as the process builds up a chain of *deferred operations* (in this case, a chain of multiplications). The contraction occurs as the operations are actually performed. This type of process, characterized by a chain of deferred operations, is called a *recursive process*. Carrying out this process requires that the interpreter keep track of the operations to be performed later on. In the computation of *n*!, the length of the chain of deffered multiplications, and hence the amount of information needed to keep track of it, grows linearly with *n* (is proportional to *n*), just like the number of steps. Such a process is called a *linear recursive process*.\n\nBy contrast, the second process does not grow and shrink. At each step, all we need to keep track of, for any *n*, are the current values of the variables `product`, `counter`, and `max-count`. We call this an *iterative process*. In general, an iterative process is one whose state can be summarized by a fixed number of *state variables*, together with a fixed rule that describes how the state variables should be updated as the process moves from state to state and an (optional) end test that specifies conditions under which the process should terminate. In computing *n*!, the number of steps required grows linearly with *n*. Such a process is called a *linear iterative process*.\n\nThe contrast between the two processes can be seen in another way. In\nthe iterative case, the program variables provide a complete\ndescription of the state of the process at any point. If we stopped the\ncomputation between steps, all we would need to do to resume the\ncomputation is to supply the interpreter with the values of the three\nprogram variables. Not so with the recursive process. In this case\nthere is some additional \"hidden\" information, maintained by the\ninterpreter and not contained in the program variables, which indicates\n\"where the process is\" in negotiating the chain of deferred operations.\nThe longer the chain, the more information must be maintained.<<When we\ndiscuss the implementation of procedures on register machines in\nchapter 5, we will see that any iterative process can be realized \"in\nhardware\" as a machine that has a fixed set of registers and no\nauxiliary memory. In contrast, realizing a recursive process requires a\nmachine that uses an auxiliary data structure known as a *stack*.>>\n\nIn contrasting iteration and recursion, we must be careful not to\nconfuse the notion of a recursive *process* and the notion of a\nrecursive *procedure*. When we describe a procedure as recursive, we\nare referring to the syntactic fact that the procedure definition\nrefers (either directly or indirectly) to the procedure itself. But\nwhen we describe a process as following a pattern that is, say,\nlinearly recursive, we are speaking about how the process evolves, not\nabout the syntax of how a procedure is written. It may seem disturbing\nthat we refer to a recursive procedure such as `fact-iter` as\ngenerating an iterative process. However, the process really is\niterative: Its state is captured completely by its three state\nvariables, and an interpreter need keep track of only three variables\nin order to execute the process.\n\nOne reason that the distinction between process and procedure may be\nconfusing is that most implementations of common languages (including\nAda, Pascal, and C) are designed in such a way that the interpretation\nof any recursive procedure consumes an amount of memory that grows with\nthe number of procedure calls, even when the process described is, in\nprinciple, iterative. As a consequence, these languages can describe\niterative processes only by resorting to special-purpose \"looping\nconstructs\" such as `do`, `repeat`, `until`, `for`, and `while`. The\nimplemenation of Scheme we shall consider in chapter 5 does not share\nthis defect. It will execute an iterative process in constant space,\neven if the iterative process is described by a recursive procedure. An\nimplementation with this property is called *tail-recursive*. With a\ntail-recursive implementation, iteration can be expressed using the\nordinary procedure call mechanism, so that special iteration constructs\nare useful only as syntactic sugar.<<Tail recursion has long been known\nas a compiler optimization trick. A coherent semantic basis for tail\nrecursion was provided by Carl Hewitt (1977), who explained it in terms\nof the \"message-passing\" model of computation that we shall discuss\nin chapter 3. Inspired by this, Gerald Jay Sussman and Guy Lewis Steele\nJr. (see Steele 1975) constructed a tail-recursive interpreter for\nScheme. Steele later showed how tail recursion is a consequence of the\nnatural way to compile procedure calls (Steele 1977). The IEEE standard\nfor Scheme requires that Scheme implementations be tail-recursive.>>\n\n> It is worthwhile to note at this point that Clojure is not\ntail-recursive.")) "" (data/exercises {:title "9-10"} (data/exercise {:ch 1, :number 9} (md {} "Each of the following two procedures defines a method for adding two\npositive integers in terms of the procedures `inc`, which increments\nits arguments by 1, and `dec`, which decrements its arguments by 1.\n\n```clj\n(defn + [a b]\n  (if (= a 0)\n      b\n      (inc (+ (dec a) b))))\n```\n```clj\n(defn + [a b]\n  (if (= a 0)\n      b\n      (+ (dec a) (inc b))))\n```\n\nUsing the substitution model, illustrate the process generated by each procedure in evaluating `(+ 4 5)`. Are these processes iterative or recursive?") (data/q-a {} (md {} "The first is recursive, and the second is iterative. Let's see why...\n\nWith the first definition,\n\n```clj\n(+ 4 5)\n(inc (+ (dec 4) 5))\n(inc (+ 3 5))\n(inc (inc (+ (dec 3) 5)))\n(inc (inc (+ 2 5)))\n(inc (inc (inc (+ (dec 2) 5))))\n(inc (inc (inc (+ 1 5))))\n(inc (inc (inc (inc (+ (dec 1) 5)))))\n(inc (inc (inc (inc (+ 0 5)))))\n(inc (inc (inc (inc 5))))\n(inc (inc (inc 6)))\n(inc (inc 7))\n(inc 8)\n9\n```\nClearly recursive in the way it grows.\n\nWith the second, however,\n\n```clj\n(+ 4 5)\n(+ (dec 4) (inc 5))\n(+ 3 6)\n(+ (dec 3) (inc 6))\n(+ 2 7)\n(+ (dec 2) (inc 7))\n(+ 1 8)\n(+ (dec 1) (inc 8))\n(+ 0 9)\n9\n```\n\nYou see the space needed to compute the second was significantly less than that of the first."))) "" (data/exercise {:ch 1, :number 10} (md {} "The following procedure computes a mathematical function called Ackermann's function.\n\n```clj\n(defn A [x y]\n  (cond (= y 0) 0\n        (= x 0) (* 2 y)\n        (= y 1) 2\n        :else (A (- x 1)\n                 (A x (- y 1)))))\n```\n\nWhat are the values of the following expressions?\n\n```clj\n(A 1 10)\n```") (data/q-a {} (md {} "```clj\n(A 1 10)\n(A (- 1 1) (A 1 (- 10 1)))\n(A 0 (A 1 9))\n(A 0 (A 0 (A 1 (- 9 1))))\n(A 0 (A 0 (A 1 8)))\n(A 0 (A 0 (A 0 (A 1 (- 8 1)))))\n(A 0 (A 0 (A 0 (A 0 (A 1 (- 7 1))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 1 (- 6 1)))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 (- 5 1))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 (- 4 1)))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 (- 3 1))))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 (- 2 1)))))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 1))))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 2)))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (* 2 2)))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 4))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (* 2 4))))))))\n(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 8)))))))\n...\n1024\n```")) "" (md {} "```clj\n(A 2 4)\n```") (data/q-a {} (md {} "```clj\n(A 2 4)\n(A (- 2 1) (A 2 (- 4 1)))\n(A 1 (A 2 3))\n(A 1 (A (- 2 1) (A 2 (- 3 1))))\n(A 1 (A 1 (A 2 2)))\n(A 1 (A 1 (A (- 2 1) (A 2 (- 2 1)))))\n(A 1 (A 1 (A 1 (A 2 1))))\n(A 1 (A 1 (A 1 2)))\n(A 1 (A 1 (A (- 1 1) (A 1 (- 2 1)))))\n(A 1 (A 1 (A 0 (A 1 1))))\n(A 1 (A 1 (A 0 2)))\n(A 1 (A 1 4))\n(A 1 (A 0 (A 1 3)))\n(A 1 (A 0 (A 0 (A 1 2))))\n(A 1 (A 0 (A 0 (A 0 (A 1 1)))))\n(A 1 (A 0 (A 0 (A 0 2))))\n(A 1 (A 0 (A 0 4)))\n(A 1 (A 0 8))\n(A 1 16)\n(A 0 (A 1 15))\n(A 0 (A 0 (A 1 14)))\n...\n65,536\n```")) (md {} "```clj\n(A 3 3)\n```") "" (data/q-a {} (md {} "```clj\n(A 3 3)\n(A 2 (A 3 2))\n(A 2 (A 2 (A 3 1)))\n(A 2 (A 2 2))\n(A 2 (A 1 (A 2 1)))\n(A 2 (A 1 2))\n(A 2 (A 0 (A 1 1)))\n(A 2 (A 0 2))\n(A 2 4)\n(A 1 (A 2 3))\n(A 1 (A 1 (A 2 2)))\n(A 1 (A 1 (A 1 (A 2 1))))\n(A 1 (A 1 (A 1 2)))\n(A 1 (A 1 (A 0 (A 1 1))))\n(A 1 (A 1 (A 0 2)))\n(A 1 (A 1 4))\n(A 1 (A 0 (A 1 3)))\n(A 1 (A 0 (A 0 (A 1 2))))\n(A 1 (A 0 (A 0 (A 0 (A 1 1)))))\n(A 1 (A 0 (A 0 (A 0 2))))\n(A 1 (A 0 (A 0 4)))\n(A 1 (A 0 8))\n(A 1 16)\n...\n65,536\n```")) (md {} "Consider the following procedures, where `A` is the procedure defined above:\n\n```clj\n(defn f [n] (A 0 n))\n```\n\n```clj\n(defn g [n] (A 1 n))\n```\n\n```clj\n(defn h [n] (A 2 n))\n```\n\n```clj\n(defn k [n] (* 5 n n))\n```\n\nGive concise mathematical definitions for the functions computed by the procedures `f`,`g`, and `h` for positive integer values of *n*. For example, `(k n)` computes \\\\(5n^2\\\\)") (data/q-a {} (md {} "`(f n)`\\\\(= 2n\\\\)\n\n`(g n)`\\\\(=2^n\\\\)\n\n`(h n)`\\\\(=2^{2^{...^{2}}}\\\\), or alternatively, `(h n)`\\\\(= 2^{(h (n-1))}\\\\)")))) (data/subsect {:title "Tree Recursion"} (md {} "Another common pattern of computation is called *tree recursion*. As an\nexample, consider computing the sequence of Fibonacci numbers, in which\neach number is the sum of the preceeding two:\n\n$$0, 1, 1, 2, 3, 5, 8, 13, 21, ...$$\n\nIn general, the Fibonacci numbers can be defined by the rule\n\n$$ \\text{fib}(n)=\\begin{cases} 0 &\\text{ if } n=0\\cr 1 &\\text{ if } n=1\n\\cr \\text{fib}(n-1)+\\text{ fib}(n-2) &\\text{ otherwise }\\end{cases}$$\n\nWe can immediately translate this definition into a recursive procedure\nfor computing Fibonacci numbers:\n\n```clj\n(defn fib [n]\n  (cond (= n 0) 0\n        (= n 1) 1\n        :else (+ (fib (- n 1))\n                 (fib (- n 2)))))\n```") "" (data/fig {:footer "The tree-recursive process generated in computing (fib 5)"}) "" (md {} "Consider the pattern of this computation. To compute `(fib 5)`, we\ncompute `(fib 4)` and `(fib 3)`. To compute `(fib 4)`, we compute `(fib\n3)` and `(fib 2)`. In general, the evolved process looks like a tree,\nas shown in [figure 1.5](#/sicp/figure/1-5/). Notice that the branches\nsplit into two at each level (except at the bottom); this reflects the\nfact that the `fib` procedure calls itself twice each time it is\ninvoked.\n\nThis procedure is instructive as a prototypical tree recursion, but it\nis a terrible way to compute Fibonacci numbers because it does so much\nredundant computation. Notice in [figure 1.5](#/sicp/figure/1-5) that\nthe entire computation of `(fib 3)` -- almost half the work -- is\ndubplicated. In fact, it is not hard to show that the number of times\nthe procedure will compute `(fib 1)` or `(fib 0)` (the number of leaves\nin the above tree, in general) is precisely *Fib(n+1)*. To get an idea\nof how bad this is, one can show that the value of *Fiv(n)* grows\nexponentially with *n*. More precisely (see [exercise\n1.13](#/sicp/ch/1/ex/13)), *Fib(n)* is the closest integer to\n\\\\(\\phi^n/\\sqrt5\\\\), where\n\n$$\\phi=\\left(1+\\sqrt5\\right)/2\\approx1.6180...$$\n\nis the *golden ratio*, which satisfies the equation\n\n$$\\phi^2=\\phi+1$$\n\nThus, the process uses a number of steps that grows exponentially with the input. On the other hand, the space required gows only linearly with the input, because we need keep track only of which nodes are above us in the tree at any point in the computation. In general, the number of steps required by a tree-recursive process will be proportional to the number of nodes in the tree, while the space requrired will be proportional to the maximum depth of the tree.\n\nWe can also formulate an iterative process for computing the Fibonacci numbers. The idea is to use a pair of integers *a* and *b*, initialized to \\\\(Fib(0)=0\\\\) and \\\\(Fib(1)=1\\\\), and to repeatedly apply the simultaneous transformations\n\n$$\\begin{align*} a \\rightarrow & b \\cr b \\rightarrow & a + b\\end{align*}$$\n\nIt is not hard to show that, after applying this transformation \\\\(n\\\\) times, \\\\(a\\\\) and \\\\(b\\\\) will be equal, respectively, to \\\\(Fib(n)\\\\) and \\\\(Fib(n+1)\\\\). Thus, we can compute Fibonacci numbers iteratively using the procedure\n\n```clj\n(defn fib-iter [a b count]\n  (if (= count 0)\n      a\n      (fib-iter b (+ a b) (- count 1))))\n(defn fib [n]\n  (fib-iter 1 0 n))\n```\n\nThis second method for computing \\\\(Fib(n)\\\\) is a linear iteration. The difference in number of steps required by the two methods --one linear in \\\\(n\\\\), one growing as fast as \\\\(Fib(n)\\\\) itself -- is enormous, even for small inputs.\n\nOne should not conclude from this that tree-recursive processes are\nuseless. When we consider processes that operate on hierarchially\nstructured data rather than numbers, we will find that tree recursion\nis a natural and powerful tool.<<An example of this was hinted at in\nsection 1.1.3: The interpreter itself evaluates expressions using a\ntree-recursive process.>> But even in numerical operations,\ntree-recursive processes can be useful in helping us to understand and\ndesign programs. For instance, although the first `fib` procedure is\nmuch less efficient than the second one, it is more straightforward,\nbeing little more than a translation into Lisp of the definition of the\nFibonacci sequence. To formulate the iterative algorithm required\nnoticing that the computation could be recast as an iteration with\nthree state variables.")) "" (data/ssub {:title "Example: Counting Change"} (md {} "It takes only a bit of cleverness to come up with the iterative Fibonacci algorithm. In contrast, consider the following problem: How many different ways can we make change of $1.00, given half-dollars, quarters, dimes, nickels, and pennies? More generally, can we write a procedure to compute the number of ways to change any given amount of money?\n\nThis problem has a simple solution as a recursive procedure. Suppose we think of the types of coins available as arranged in some order. Then the following relation holds:\n\nThe number of ways to change amount \\\\(a\\\\) using \\\\(n\\\\) kinds of coins equals\n\n- \n  The number of ways to change amount \\\\(a\\\\) using all but the first kind of coin, plus\n\n- \n  The number of ways to change amount \\\\(a-d\\\\) using all \\\\(n\\\\) kinds of coins, where \\\\(d\\\\) is the denomination of the first kind of coin.\n\nTo see why this is true, observe that the ways to make change can be divided into two groups: those that do not use any of the first kind, and those that do. Therefore, the total number of ways to make change for some amount is equal to the number of ways to make change for the amount without using any of the first kind of coin, plus the number of ways to make change assuming that we do use the first kind of coin. But the latter number is equal to the number of ways to make change for the amount that remains after using a coin of the first kind.\n\nThus, we can recursively reduce the problem of changing a give amount\nto the problem of changing smaller amounts using fewer kinds of coins.\nConsider this reduction rule carefully, and convince yourself that we\ncan use it to describe an algorithm if we specify the following\ndegenerate cases:<<For example, work through in detail how the\nreduction rule applies to the problem of making change for 10 cents\nusing pennies and nickels.>>\n\n- If \\\\(a\\\\) is exactly 0, we should count that as 1 way to make change.\n\n- If \\\\(a\\\\) is less than 0, we should count that as 0 ways to make change.\n\n- If \\\\(n\\\\) is 0, we should count that as 0 ways to make change.\n\nWe can easily translate this description into a recursive procedure:\n\n```clj\n(defn count-change [amount]\n  (cc amount 5))\n```\ngiven that we've defined\n\n```clj\n(defn cc [amount kinds-of-coins]\n  (cond (= amount 0) 1\n        (or (< amount 0) (= kinds-of-coins 0)) 0\n        :else (+ (cc amount (- kinds-of-coins 1))\n                 (cc (- amount (first-denomination kinds-of-coins)) kinds-of-coins))))\n```\n\nand\n\n```clj\n(defn first-denomination [kinds-of-coins]\n  (cond (= kinds-of-coins 1) 1\n        (= kinds-of-coins 2) 5\n        (= kinds-of-coins 3) 10\n        (= kinds-of-coins 4) 25\n        (= kinds-of-coins 5) 50))\n```\n\n> Note that I made the stipulation that we've already defined the two next things. I said this because the REPL we're using for this book will return an error if you use somthing that has not been previously defined, so if we try to use `first-denomination` in a function definition before we've defined `first-denomination`, the program won't compile. I introduced the functions in the order I did for two reasons: 1) it's the same order that is used in the original SICP, and 2) it makes more sense pedagogically.\n\n(The `first-denomination` procedure takes as input the number of kinds\nof coins available and returns the denomination of the first kind. Here\nwe are thinking of the coins as arranged in order from largest to\nsmallest, but any order would do as well.) We can now answer our\noriginal question about changing a dollar:\n\n```clj\n(count-change 100)\n292\n```\n\n`count-change` generates a tree-recursive process with redundancies\nsimilar to those in our first implementation of `fib`. (It will take\nquite a wile for that 292 to be computed.) On the other hand, it is not\nobvious hwo to design a better algorithm for computing the result, and\nwe leave this problem as a challenge. The observation that a\ntree-recursive process may be highly inefficient but often easy to\nspecify and understand has led people to propose that one could get the\nbest of both worlds by designing a \"smart compiler\" that could\ntransform tree-recusive procedures into more efficient procedures that\ncompute the same result.<<One approach to coping with redundant\ncomputations is to arrange matters so that we automatically construct a\ntable of values as they are computed. Each time we are asked to apply\nthe procedure to some argument, we first look to see if the value is\nalready stored in the table, in which case we avoid performing the\nredundant computation. This strategy, known as *tabulation* or\n*memoization*, can be implemented in a straightforward way. Tabulation\ncan sometimes be used to transform processes that require an\nexponential number of steps (such as `count-change`) into processes\nwhose space and time requirements grow linearly with the input. See\nexercise 3.27.>>")) "" (data/exercises {:title "11-13"} (data/exercise {:ch 1, :number 11} (md {} "A function \\\\(f\\\\) is defined by the rule that\n\\\\(f(n)=f(n-1)+2f(n-2)+3f(n-3)\\\\) if \\\\(n\\geq3\\\\). Write a procedure\nthat computes \\\\(f\\\\) by means of a recursive process. Write a\nprocedure that computes \\\\(f\\\\) by a means of an iterative process.") "" (data/q-a {} (md {} "Recursive:\n\n```clj\n(defn f [n]\n  (if (< n 3)\n      n\n      (+ (f (- n 1)) \n         (* 2 (f (- n 2))) \n         (* 3 (f (- n 3))))))\n```\nIterative:\n\n```clj\n(defn f-iter [i a b c]\n  (if (> i n)\n      a\n      (f-iter (inc i) (+ a (* 2 b) (* 3 c)) a b)))\n(defn f [n]\n  (if (< n 3)\n      n\n      (f-iter 2 2 1 0)))\n```"))) (data/exercise {:ch 1, :number 12} (md {} "The following patter is called *Pascal's Triangle*.") (data/image {:src "img/sicp/exercise1_12.gif"}) (md {} "The numbers at the edge of the triangle are all 1, and each number\ninside the triangle is the sum of the two numbers above it.<<The\nelements of Pascal's triangle are called the *binomial coefficients*,\nbecause the \\\\(n\\\\)th row consists of the coefficients of the terms\nin the expansion of \\\\((x + y)^n\\\\). This pattern for computing the\ncoefficients appeared in Blaise Pascal's 1653 seminal work on\nprobability theory, *Traité du triangle arithmétique*. According to\nKnuth (1973), the same pattern appears in the *Szu-yuen Yü-chien*\n(\"The Precious Mirror of the Four Elements\"), published by the\nChinese mathematician Chu Shih-chieh in 1303, in the works of the\ntwelfth-century Persian poet and mathematician Omar Khayyam, and in\nthe works of the twelfth-century Hindu mathematician Bháscara\nÁchárya.>> Write a procedure that computes elements of Pascal's\nTriangle by means of a recursive process.") "" (data/q-a {} (md {} "```clj\n(defn pascal [row column]\n  (cond (or (< row column) (< row 1) (< column 1)) 0\n        (or (= row column) (= column 1)) 1\n        :else (+ (pascal (- row 1) (- column 1))\n                 (pascal (- row 1) column))))\n```"))) "" (data/exercise {:ch 1, :number 13} (md {} "Prove that \\\\(Fib(n)\\\\) is the closest integer to\n\\\\(\\phi^n/\\sqrt5\\\\), where \\\\(\\phi=\\left(1+\\sqrt5\\right)/2\\\\). Hint:\nlet \\\\(\\psi=\\left(1-\\sqrt5\\right)/2\\\\) Use induction and the\ndefinition of the Fibonacci numbers (see [section\n1.2.2](#/sicp/ch/1/sect/2/sub/2)) to prove that\n\\\\(Fib(n)=\\left(\\phi^n-\\psi^n\\right)/\\sqrt5\\\\).") "" (data/q-a {} (md {} "Let's pick a natural number \\\\(n\\\\) and state our inductive hypothesis:\n\nAssume that for any \\\\(k< n\\\\),\n\\\\(Fib(k)=\\left(\\phi^k-\\psi^k\\right)/\\sqrt5\\\\).\n\nCase 1: \\\\(n=0\\\\).\n\nThen note: \\\\( Fib(0)= 0\n=\\frac{1-1}{\\sqrt5}=\\frac{\\phi^0-\\psi^0}{\\sqrt5}\\\\).\n\nCase 2: \\\\( n = 1\\\\)\n\nThen \\\\(\\frac{\\phi^1-\\psi^1}{\\sqrt5}\n=\\frac{\\left(\\frac{1+\\sqrt5}{2}\\right)^1-\\left(\\frac{1-\\sqrt5}{2}\\right)^1}{\\sqrt5}\n=\\frac{\\sqrt5}{\\sqrt5}= 1= Fib(1)\\\\)\n\nOur last case is if \\\\(n > 1\\\\):\n\nThen, $$\\frac{\\phi^n-\\psi^n}{\\sqrt5}\n=\\frac{\\phi^{n-2}\\phi^2-\\psi^{n-2}\\psi^2}{\\sqrt5}$$\n\n$$=\\frac{\\phi^{n-2}\\left(\\frac{1+\\sqrt5}{2}\\right)^2-\\psi^{n-2}\\left(\\frac{1-\\sqrt5}{2}\\right)^2}{\\sqrt5}$$\n\n$$=\\frac{\\phi^{n-2}\\left(\\frac{6+2\\sqrt5}{4}\\right)-\\psi^{n-2}\\left(\\frac{6-2\\sqrt5}{4}\\right)}{\\sqrt5}$$ \n\n$$=\\frac{\\phi^{n-2}-\\psi^{n-2}}{\\sqrt5}+\\frac{\\phi^{n-2}\\left(\\frac{1+\\sqrt5}{2}\\right)-\\psi^{n-2}\\left(\\frac{1-2\\sqrt5}{2}\\right)}{\\sqrt5}$$\n\n$$=Fib(n-2)+\\frac{\\phi^{n-1}-\\psi^{n-1}}{\\sqrt5}$$\n\n$$=Fib(n-2)+Fib(n-1)$$\n\n$$=Fib(n)$$\n\nSo, in general, \\\\(Fib(n)=\\frac{\\phi^n-\\psi^n}{\\sqrt5}\\\\)\n\nNow, to prove that \\\\(Fib(n)\\\\) is the integer closest to\n\\\\(\\phi^n/\\sqrt5\\\\). First we note that since \\\\(\\psi^2< 0.4\\\\),\n\\\\(\\sqrt5>1\\\\) and \\\\(\\frac{\\phi^n-\\psi^n}{\\sqrt5}\\\\) is an\ninteger, we know for any \\\\(n\\geq2\\\\), \n\n$$\\left|\\frac{\\phi^n}{\\sqrt5}-Fib(n)\\right|\n=\\left|\\frac{\\phi^n}{\\sqrt5}-\\frac{\\phi^n-\\psi^n}{\\sqrt5}\\right|\n=\\left|\\frac{\\psi^n}{\\sqrt5}\\right|< 0.4$$\n\nSo that the closest integer to \\\\(\\frac{\\phi^n}{\\sqrt5}\\\\) must be\n\\\\(Fib(n)\\\\).\n\nAs for when \\\\(n< 2\\\\), we can check one by one.\n\nCase \\\\(n = 0\\\\): \\\\(\\frac{1}{\\sqrt5}< 0.45\\\\), so that the closest\ninteger is 0 (\\\\(Fib(0)\\\\)).\n\nCase \\\\(n=1\\\\): \\\\(\\frac{\\frac{1+\\sqrt5}{2}}{\\sqrt5}\\approx0.72\\\\),\nso that the closest integer is \\\\(Fib(1)\\\\).")))) "" (data/subsect {:title "Orders of Growth"} (md {} "The previous examples illustrate that processes can differ considerably\nin the rates at which they consume computational resources. One\nconvenient way to describe this difference is to use the notion of\n*order of growth* to obtain a gross measure of the resources required\nby a process as the inputs become larger.\n\nLet \\\\(n\\\\) be a parameter that measures the size of the problem, and\nlet \\\\(R(n)\\\\) be the amount of resources the process requires for a\nproblem of size \\\\(n\\\\). In our previous examples we took \\\\(n\\\\) to be\nthe number for which a given function is to be computed, but there are\nother possibilities. For instance, if our goal is to compute an\napproximation to the square root of a number, we might take \\\\(n\\\\) to\nbe the number of digits accuracy required. For matrix multiplication we\nmight take \\\\(n\\\\) to be the number of rows in the matrices. In general\nthere are a number of properties of the problem with respect to which\nit will be desirable to analyze a given process. Similarly, \\\\(R(n)\\\\)\nmight measure the number of internal storage registers used, the number\nof elementary machine operations performed, and so on. In computers\nthat do only a fixed number of operations at a time, the time required\nwill be proportional to the number of elementary machine operations\nperformed.\n\nWe say \\\\(R(n)\\\\) has order of growth \\\\(\\Theta(f(n))\\\\), written\n\\\\(R(n)=\\Theta(f(n))\\\\) (pronounced \"theta of \\\\(f(n)\\\\)\"), if there\nare positive constants \\\\(k_1\\\\) and \\\\(k_2\\\\) independent of \\\\(n\\\\)\nsuch that\n\n$$k_1f(n)\\leq R(n)\\leq k_2f(n)$$\n\nfor any sufficiently large value of \\\\(n\\\\). (In other words, for large\n\\\\(n\\\\), the value of \\\\(R(n)\\\\) is sandwiched between \\\\(k_1f(n)\\\\)\nand \\\\(k_2f(n)\\\\).)\n\nFor instance, with the linear recursive process for computing factorial\ndescribed in [section 1.2.1](#/sicp/ch/1/sect/2/sub/1/) the number of\nsteps grows proportionally to the input \\\\(n\\\\). Thus, the steps\nrequired for this process grows as \\\\(\\Theta(n)\\\\). We also saw that\nthe space required grows as \\\\(\\Theta(n)\\\\). For the iterative\nfactorial, the number of steps is still \\\\(\\Theta(n)\\\\) but the space\nis \\\\(\\Theta(1)\\\\) -- that is, constant.<<These statements mask a great\ndeal of oversimplification. For instance, if we count process steps as\n\"machine operations\" we are making the assumption that the number of\nmachine operations needed to perform, say, a multiplication is\nindependent of the size of the numbers to be multiplied, which is false\nif the numbers are sufficiently large. Similar remarks hold for the\nestimates of space. Like the design and description of a process, the\nanalysis of a process can be carried out at various levels of\nabstraction.>> The tree-recursive Fibonacci computation requires\n\\\\(\\Theta(\\phi^n)\\\\) steps and space \\\\(\\Theta(n)\\\\), where \\\\(\\phi\\\\)\nis the golden ratio described in [section\n1.2.2](#/sicp/ch/1/sect/2/sub/2/).\n\nOrders of growth provide only a crude description of the behavior of a\nprocess. For example, a process requiring \\\\(n^2\\\\) steps and a process\nrequiring \\\\(1000n^2\\\\) steps and a process requiring \\\\(3n^2+10n+17\\\\)\nsteps all have \\\\(\\Theta(n^2)\\\\) order of growth. On the other hand,\norder of growth provides a useful indication of how we may expect the\nbehavior of the process to change as we change the size of the problem.\nFor a \\\\(\\Theta(n)\\\\) (linear) process, doubling the size will roughly\ndouble the amount of resources used. For an exponential process, each\nincrement in problem size will multiply the resource utilization by a\nconstant factor. In the remainder of section 1.2 we will examine two\nalgorithms whose order of growth is logarithmic, so that doubling the\nproblem size increases the resource requirment by a constant amount.")) "" (data/exercises {:title "14-15"} (data/exercise {:ch 1, :number 14} (md {} "Draw the tree illustrating the process generated by the\n`count-change` procedure of [section\n1.2.2](#/sicp/ch/1/sect/2/sub/2/) in making change for 11 cents. What\nare the orders of growth of the space and number of steps used by\nthis process as the amount to be changed increases?") "" (data/q-a {} (md {} "Things will go here (once I get this LaTeX thing all sorted out)."))) "" (data/exercise {:ch 1, :number 15} (md {} "The sine of an angle (specified in radians) can be computed by making\nuse of the approximation \\\\(\\sin x\\approx x\\\\) if \\\\(x\\\\) is\nsufficiently small, and the trigonometric identity\n\n$$\\sin x = 3\\sin\\frac{x}{3}-4\\sin^3\\frac{x}{3}$$\n\nto reduce the size of the argument of \\\\(\\sin\\\\). (For purposes of\nthis exerise an angle is considered \"sufficiently small\" if its\nmagnitude is not greater than 0.1 radians.) These ideas are\nincorporated in the following procedures:\n\n```clj\n(defn cub [x] (* x x x))\n(defn p [x] (- (* 3 x) (* 4 (cube x))))\n(defn sine [angle]\n  (if (not (> (abs angle) 0.1))\n      angle\n      (p (sine (/ angle 3.0)))))\n```\n\na. How many times is the procedure \\\\(p\\\\) applied when `(sine\n12.15)` is evaluated?") "" (data/q-a {} (md {} "To do this we first realize that each iteration (until the angle\n\\\\(\\theta\\\\) is less than or equal to 0.1) divides the angle by\nthree. Since this is applicative order, we know that each iteration\nwill call `p` exactly once. So this reduces to solving the\nequation,\n\n$$ \\frac{12.15}{3^n}\\leq 0.1 \\iff n\\geq 4.37... $$\n\nSince we're looking for an integer, we need \\\\(n=5\\\\). Hence, `p`\nwill be applied 5 times.")) "" (md {} "b. What is the order of growth in space and number of steps (as a\nfunction of \\\\(a\\\\)) used by the process generated by the `sine`\nprocedure when `(sine a)` is evaluated?") "" (data/q-a {} (md {} "One could realize, just from noting that each step divides the argument by 3, that the order of growth in the number of steps required is logarithmic (doubling the argument only adds at most one more iteration, and hence a fixed number of steps).\n\nAs far as in the space required, it is also logarithmic because every time you triple the argument, you only add one more iteration.\n\nSo both have \\\\(\\Theta(\\log x)\\\\) order of growth.")))) (data/ssub {:title "Exponentiation"} (md {} "Consider the problem of computing the exponential of a given number. We would like a procedure that takes as arguments a base \\\\(b\\\\) and a positive integer exponent \\\\(n\\\\) and computes \\\\(b^n\\\\). One way to do this is via the recursive definition:\n\n$$\\begin{align*}b^n =& b\\cdot b^{n-1} \\\\\\\\ b^0=&1 \\end{align*}$$\n\nWhich translates readily into the procedure\n\n```clj\n(define expt [b n]\n  (if (= n 0)\n      1\n      (* b (expt b (- n 1)))))\n```\n\nThis is a linear recursive process, which requires \\\\(\\Theta(n)\\\\) steps and \\\\(\\Theta(n)\\\\) space. Just as with factorial, we can readily formulate an equivalent linear iteration:\n\n```clj\n(defn expt [b n]\n  (expt-iter b n 1))\n```\n\nwhere\n\n```clj\n(defn expt-iter [b counter product]\n  (if (= counter 0)\n      product\n      (expt-iter b\n                 (- counter 1)\n                 (* b product))))\n```\n\nThis version requires \\\\(\\Theta(n)\\\\) steps and \\\\(\\Theta(1)\\\\) space.\n\nWe can compute exponentials in fewer steps by using successive\nsquaring. For instance, rather than computing \\\\(b^8\\\\) as\n\\\\(b\\cdot(b\\cdot(b\\cdot(b\\cdot(b\\cdot(b\\cdot(b\\cdot b))))))\\\\) we can\ncompute it using three multiplications:\n\n$$\\begin{align*} b^2=& b\\cdot b\n  \\\\\\\\ b^4=& b^2\\cdot b^2\n  \\\\\\\\ b^8=& b^4\\cdot b^4\\end{align*}$$\n\nThis method works fine for exponents that are powers of 2. We can also\ntake advantage of successive squaring in computing exponentials in\ngeneral if we use the rule:\n\n$$\\begin{align*}\n  b^n=&(b^{n/2})^2 & \\text{if }& n \\text{ is even} \\\\\\\\\n  b^n=&b\\cdot b^{n-1} & \\text{if }& n \\text{ is odd}\\end{align*}$$\n\nWe can express this method as a procedure:\n\n```clj\n(defn even? [n]\n  (= (mod n 2) 0))\n```\n\n> Note: In Clojure `mod` takes the place of `remainder` in Scheme.\n\n\nThe process evolved by `fast-expt` grows logarithmically with \\\\(n\\\\)\nin both space and number of steps. To see this, observe that computing\n\\\\(b^{2n}\\\\) using `fast-expt` requires only one more multiplication\nthan computing \\\\(b^n\\\\). The size of the exponent we can compute\ntherefore doubles (approximately) with every new multiplication we are\nallowed. Thus the number of multiplications required for an exponent of\n\\\\(n\\\\) grows about as fast as the logarithm of \\\\(n\\\\) to the base 2.\nThe process has \\\\(\\Theta(\\log n)\\\\) growth.<<More precisely, the\nnumber of multiplications required is equal to 1 less than the log base\n2 of \\\\(n\\\\) plus the number of ones in the binary representation of\n\\\\(n\\\\).  This total is always less than twice \\\\(\\log_2(n)\\\\). The\narbitrary constants \\\\(k_1\\\\) and \\\\(k_2\\\\) in the definition of order\nnotation imply that, for a logarithmic process, the base to which\nlogarithms are taken does not matter, so all such processes are\ndescribed as \\\\(\\Theta(\\log n)\\\\).>>\n\nThe difference between \\\\(\\Theta(\\log n) \\\\) growth and \\\\(\\Theta(n)\n\\\\) growth becomes striking as \\\\(n \\\\) becomes large. For example,\n`fast-expt` for \\\\(n=1000 \\\\) requires only 14 multiplications.\n<<You may wonder why anyone would care about raising numbers to the\n1000th power. See section 1.26.>> It is also possible to use the idea\nof successive squaring to devise an iterative algorithm that computes\nexponentials with a logarithmic number of steps (see [exercise\n1.16](#/sicp/ex/1-16/)), although, as is often the case with iterative\nalgorithms, this is not written down so straightforwardly as the\nrecursive algorithm.<<This iterative algorithm is ancient. It appears\nin the *Chandah-sutra* by Áchárya Pingala, written before 200 B.C. See\nKnuth 1981, section 4.6.3, for a full discussion and analysis of this\nand other methods of exponentiation.>>")) "" (data/exercises {:title "16-19"} (data/exercise {:ch 1, :number 16} (md {} "Design a procedure that evolves an iterative exponentiation process that uses successive squaring and uses a logarithmic number of steps, as does `fast-expt`. (Hint: Using the observations that \\\\((b^{n/2})^2=(b^2)^{n/2}\\\\), keep, along with the exponent \\\\(n\\\\) and the base \\\\(b\\\\), an additional state variable \\\\(a\\\\), and define the state transformation in such a way that the product \\\\(ab^n\\\\) is unchanged from state to state. At the beginning of the process \\\\(a\\\\) is taken to be 1, and the answer is given by the value of \\\\(a\\\\) at the end of the process. In general, the technique of defining an *invariant quantity* that remains unchanged from state to state is a powerful way to think about the design of iterative algorithms.)") "" (data/q-a {} (md {} "```clj\n(defn expt [b n]\n  (loop [base     b\n         exponent n\n         accumulator 1]\n    (cond (= exponent 0) accumulator\n          (= (mod exponent 2) 0) (recur (square base) (/ exponent 2) accumulator)\n          :else (recur base (- exponent 1) (* base accumulator)))))\n```\n\nNote: Due to the fact that Clojure is not tail-recursive, in order\nto avoid a stack overflow, we want to avoid recursion whenever\npossible. These are the perils of working with a hosted language."))) "" (data/exercise {:ch 1, :number 17} (md {} "The exponentiation algorithms in this section are based on performing exponentiation by means of repeated multiplication. In a similar way, one can perform integer multiplication by means of repeated addition. The following multiplication procedure (in which it is assumed that our language can only add, not multiply) is analogous to the `expt` procedure:\n\n```clj\n(defn * [a b]\n  (if (= b 0)\n      0\n      (+ a (* a (- b 1)))))\n```\n\nThis algorithm takes a number of steps that is linear in `b`. Now suppose we include, together with addition, operations `double`, which doubles an integer, and `halve`, which divides an (even) integer by 2. Using these, design a multiplication procedure analogous to `fast-expt` that uses a logarithmic number of steps.") (data/q-a {} (md {} "```clj\n(defn fast-mult [a b]\n  (loop [thing1 a\n         thing2 b\n         accu   0]\n    (cond (= thing1 0) accu\n          (= (mod thing1 2) 0) (recur (halve thing1) (double thing2) accu)\n          :else (recur (- thing1 1) thing2 (+ thing2 accu)))))\n```"))) "" (data/exercise {:ch 1, :number 18} (md {} "Using the results of exercises 1.16 and 1.17, devise a procedure that\ngenerates an iterative process for multiplying two integers in terms\nof adding, doubling, and halving and uses a logarithmic number of\nsteps.<<This algorithm, which is sometimes known as the \"Russian\npeasant method\" of multiplication, is ancient. Examples of its use\nare found in the Rhind Papyrus, one of the two oldest mathematical\ndocuments in existence, written about 1700 B.C. (and copied from an\neven older document) by an Egyptian scribe named A'h-mose.>>") "" (data/q-a {} (md {} "It's exactly the same as the one above."))) "" (data/exercise {:ch 1, :number 19} (md {} "There is a clever algorithm for computing the Fibonacci numbers in a\nlogarithmic number of steps. Recall the transformation of the state\nvariables \\\\(a\\\\) and \\\\(b\\\\) in the `fib-iter` process of [section\n1.2.2](#!/sicp/ch/1/sect/2/sub/2/): \\\\(a\\to a + b\\\\) and \\\\(b\\to\na\\\\). Call this transformation \\\\(T\\\\), and observe that applying\n\\\\(T\\\\) over and over again \\\\(n\\\\) times, starting with 0 and 1,\nproduces the pair \\\\(Fib(n)\\\\) and \\\\(Fib(n+1)\\\\). In other words,\nthe Fibonacci numbers are produced by applying \\\\(T^n\\\\), the nth\npower of the transformation \\\\(T\\\\), starting with the pair (0,1).\nNow consider \\\\(T\\\\) to be the special case of \\\\(p = 0\\\\) and \\\\(q =\n1\\\\) in a family of transformations \\\\(T_{pq}\\\\), where \\\\(T_{pq}\\\\)\ntransforms the pair (a,b) according to \\\\(a\\to ap + bq\\\\) and \\\\(b\\to\naq + bq + bp\\\\) (so that \\\\(T_{pq}(a,b)=(ap+bq,aq+bq+bp)\\\\)). Show\nthat if we apply such a transformation \\\\(T_{pq}\\\\) twice, the effect\nis the same as using a single transformation \\\\(T_{p’q’}\\\\) of the\nsame form, and compute \\\\(p’\\\\) and \\\\(q’\\\\) in terms of \\\\(p\\\\) and\n\\\\(q\\\\). This gives us an explicit way to square these\ntransformations, and thus we can compute \\\\(T^n\\\\) using successive\nsquaring, as in the `fast-expt` procedure. Put this all together to\ncomplete the following procedure, which runs in a logarithmic number\nof steps:<<This exercise was suggested to us by Joe Stoy, based on an example in Kaldewaij 1990.>>\n\n```clj\n(defn fib [n]\n  (loop [p 0\n         q 1\n         a 0\n         b 1\n         count n]\n    (cond (= count 0) a\n          (even? count) (recur <??>  ; compute p'\n                               <??>  ; compute q'\n                               a\n                               b\n                               (/ count 2))\n          :else (recur p\n                       q\n                       (+ (* a p) (* b q))\n                       (+ (* a q) (* b q) (* b p))\n                       (dec count)))))\n```") "" (data/q-a {} (md {} "Let's find what \\\\(p',q'\\\\) are.\n\n$$\\begin{align*}T_{pq}^2(a,b)=&T_{pq}(ap+bq,aq+bq+bp)\n\\\\\\\\=&((ap+bq)p+(aq+bq+bp)q,(ap+bq)q+(aq+bq+bp)(q+p))\n\\\\\\\\=&(ap^2+bpq+aq^2+bq^2+bpq, apq+bq^2+aq^2+apq+b(q+p)^2)\n\\\\\\\\=&(a(p^2+q^2)+b(q^2+2pq),a(q^2+2pq)+b(p^2+2q^2+2pq))\\end{align*}$$\n\nSo that, \\\\(p'=p^2+q^2\\\\) and \\\\(q'=q^2+2pq\\\\) and our missing\npieces of code are, `(+ (square p) (square q))` and `(+ (square q)\n(* 2 p q))` respectively.")))) (data/subsect {:title "Greatest Common Divisors"} (md {} "The greatest common divisor (GCD) of two integers \\\\(a\\\\) and \\\\(b\\\\)\nis defined to be the largest integer that divides both \\\\(a\\\\) and\n\\\\(b\\\\) with no remainder. For example, the GCD of 16 and 28 is 4. In\nchapter 2, when we investigate how to implement rational-number\narithmetic, we will need to be able to compute GCDs in order to reduce\nrational numbers to lowest terms. (To reduce a rational number to\nlowest terms, we must divide both the numerator and the denominator by\ntheir GCD. For example, 16/28 reduces to 4/7.) One way to find the GCD\nof two integers is to factor them and search for common factors, but\nthere is a famous algorithm that is much more efficient.\n\nThe idea of the algorithm is based on the observation that, if \\\\(r\\\\) is the remainder when \\\\(a\\\\) is divided by \\\\(b\\\\), then the common divisors of \\\\(a\\\\) and \\\\(b\\\\) are precisely the same as the common divisors of \\\\(b\\\\) and \\\\(r\\\\). Thus, we can use the equation\n\n$$GCD(a,b)=GCD(b,r)$$\n\nto successively reduce the problem of computing a GCD to the problem of computing the GCD of smaller and smaller pairs of integers. For example,\n\n$$\\begin{align*}\n  GCD(206,40)=&GCD(40,6)\n         \\\\\\\\=&GCD(6,4)\n         \\\\\\\\=&GCD(4,2)\n         \\\\\\\\=&GCD(2,0)\n         \\\\\\\\=&2\n  \\end{align*}$$\n\nreduces GCD(206,40) to GCD(2,0), which is 2. It is possible to show\nthat starting with any two positive integers and performing repeated\nreductions will always eventually produce a pair where the second\nnumber is 0. Then the GCD is the other number in the pair. This method\nfor computing the GCD is known as *Euclid's Algorithm*.<<Euclid's\nAlgorithm is so called because it appears in Euclid's *Elements* (Book\n7, ca. 300 B.C.). According to Knuth (1973), it can be considered the\noldest known nontrivial algorithm. The ancient Egyptian method of\nmultiplication (exercise 1.18) is surely older, but, as Knuth explains,\nEuclid's algorithm is the oldest known to have been presented as a\ngeneral algorithm, rather than as a set of illustrative examples.>>\n\nIt is easy to express Euclid's Algorithm as a procedure:\n\n```clj\n(defn gcd [a b]\n  (if (= b 0)\n      a\n      (gcd b (mod a b))))\n```\n\nOr using recur, \n\n```clj\n(defn gcd [a b]\n  (if (= b 0)\n      a\n      (recur b (mod a b))))\n```\n\nThis generates an iterative process, whose number of steps grows as the logarithm of the numbers involved.\n\nThe fact that the number of steps required by Euclid's Algorithm has a logarithmic growth bears an interesting relation to the Fibonacci numbers:\n\n**Lamé's Theorem:** If Euclid's Algorithm requires \\\\(k\\\\) steps to\ncompute the GCD of some pair, then the smaller number in the pair must\nbe greater than or equal to the \\\\(k\\\\)th Fibonacci number.<<We can\nprove this by induction. Let's pick an arbitrary natural number\n\\\\(n\\\\), and assume that for any \\\\(k<n\\\\), if a pair of natural\nnumbers \\\\(a>b>0\\\\) requires \\\\(k\\\\) steps of Euclid's algorithm to\ncompute their GCD, then both \\\\(a\\\\) and \\\\(b\\\\) are greater than or\nequal to \\\\(Fib(k)\\\\). Now we take a pair, \\\\((a_n,b_n)\\\\) that takes\n\\\\(n\\\\) steps. If \\\\(n=0\\\\), then we know we started with zero and\nwe're done. If \\\\(n=1\\\\), then \\\\(b|a\\\\), and since neither of\nthem are 0, we're done. Otherwise, we note that every time we apply an\niteration of the algorithm \\\\((a_n,b_n)\\to(a_{n-1},b_{n-1})\\\\), where\n\\\\(a_{n-1}=b_n\\\\) and \\\\(a_n=q\\cdot b_n+b_{n-1}\\\\) for some integer\n\\\\(q>0\\\\). Now we note, if we apply two iterations, \\\\((a_n,b_n)\\to\n(a_{n-2},b_{n-2})\\\\), where \\\\(a_{n-2}=b_{n-1}\\\\) and \\\\(a_{n-1}=s\\cdot\nb_{n-1}+b_{n-2}\\\\) and \\\\(s>0\\\\). So we've concluded\n\\\\(b_n=a_{n-1}=s\\cdot b_{n-1}+b_{n-2}\\geq b_{n-1}+b_{n-2}\\\\), which by\nassumption are greater than \\\\(Fib(n-1)\\\\) and \\\\(Fib(n-2)\\\\)\nrespectively. Hence, \\\\(a_n>b_n\\geq Fib(n-1)+Fib(n-2)=Fib(n)\\\\).>>\n\nWe can use this theorem to get an order-of-growth estimate for Euclid's\nAlgorithm. Let \\\\(n\\\\) be the smaller of the two inputs to the\nprocedure. If the process takes \\\\(k\\\\) steps, then we must have\n\\\\(n\\geq Fib(k)\\approx\\phi^k/\\sqrt5\\\\). Therefore the smaller of steps\n\\\\(k\\\\) grows as the logarithm (to the base \\\\(\\phi\\\\)) of \\\\(n\\\\).\nHence, the order of growth is \\\\(\\Theta(\\log n)\\\\).")) "" (data/exercises {:title "20"} (data/exercise {} (md {} "The process that a procedure generates is of course dependent on the\nrules used by the interpreter. As an example, consider the iterative\n`gcd` procedure given above. Suppose we were to interpret this\nprocedure using normal-order evaluation, as discussed in [section\n1.5](#!/sicp/ch/1/sect/5/). (The normal-order-evaluation rule for `if`\nis described in exercise 1.5.) Using the substitution method (for\nnormal order), illustrate the process generated in evaluating `(gcd 206\n40)` and indicate the `mod` operations that are actually performed. How\nmany `mod` operations are actually performed in the normal-order\nevaluation of `(gcd 206 40)`? In the applicative-order evaluation?") "" (data/q-a {} (md {} "For applicative order evaluation:\n\n```clj\n(GCD 206 40) = (GCD 40 (mod 206 40))\n             = (GCD 40 6)\n             = (GCD 6 (mod 40 6))\n             = (GCD 6 4\n             = (GCD 4 (mod 6 4))\n             = (GCD 4 2)\n             = (GCD 2 (mod 6 2))\n             = (GCD 2 0)\n             = 2\n```\n\nSo, in applicative order, `mod` is only called four times.\n\nFor normal-order evaluation it would start with `(= 40 0)` which\nwould lead it to `(GCD 40 (mod 206 40))` without computing the `mod`\nyet. Then it evaluates `(= (mod 206 40) 0)` with our first `mod`\nevaluation. Then it calls `(GCD (mod 206 40) (mod 40 (mod 206 40)))`\nwithout computing the `mods`, which evaluates `(= (mod 40 (mod 206\n40)) 0)` which gives us our second and third calls for `mod` and\ncalls `(GCD (mod 40 (mod 206 40)) (mod (mod 206 40) (mod 40 (mod 206\n40))))` without evaluating the arguments. This calls `(= (mod (mod\n206 40) (mod 40 (mod 206 40))) 0)` and so on.")))) "" (data/subsect {:title "Example: Testing for Primality"} (md {} "This section describes two methods for checking the primality of an\ninteger \\\\(n\\\\), one with order of growth \\\\(\\Theta(\\sqrt n)\\\\), and a\n\"probabilistic\" algorithm with order of growth \\\\(\\Theta(\\log n)\\\\).\nThe exercises at the end of this section suggest programming projects\nbased on these algorithms.")) "" (data/ssub {:title "Searching for Divisors"} (md {} "Since ancient times, mathematicians have been fascinated by the\nproblems concerning prime numbers, and many people have worked on the\nproblem of determining ways to test if numbers are prime. One way to\ntest if a number is prime is to find the number's divisors. The\nfollowing program finds the smallest integral divisor (greater than 1)\nof a given number \\\\(n\\\\). It does this in a straightforward way, by\ntesting \\\\(n\\\\) for divisibility by successive integers starting with\n2.\n\n```clj\n(defn divides? [a b]\n  (= (mod b a) 0))\n(defn find-divisor [n test-divisor]\n  (cond (> (square test-divisor) n) n\n        (divides? test-divisor n) test-divisor\n        :else (recur n (inc test-divisor))))\n(defn smallest-divisor [n]\n  (find-divisor n 2))\n```\n\nWe can test whether a number is prime as follows: \\\\(n\\\\) is prime if and only if \\\\(n\\\\) is its own smallest divisor.\n\n```clj\n(defn prime? [n]\n  (= n (smallest-divisor n)))\n```\n\nThe end test for `find-divisor` is based on the fact that if \\\\(n\\\\) is\nnot prime it must have a divisor less than or equal to \\\\(\\sqrt\nn\\\\).<<If \\\\(d\\\\) is a divisor of \\\\(n\\\\), then so id \\\\(n/d\\\\). But\n\\\\(d\\\\) and \\\\(n/d\\\\) cannot both be greater than \\\\(\\sqrt n\\\\).>> This\nmeans that the algorithm need only test divisors between 1 and \\\\(\\sqrt\nn\\\\). Consequently, the number of steps required to identify \\\\(n\\\\) as\nprime will have order of growth \\\\(\\Theta (\\sqrt n)\\\\).")) "" (data/ssub {:title "The Fermat Test"} (md {} "The \\\\(\\Theta(\\log n)\\\\) primality test is based on a result from\nnumber theory known as Fermat's Little Theorem.<<Pierre de Fermat\n(1601-1665) is considered to be the founder of modern number theory. He\nobtained many important number-theoretic results, but he usually\nannounced just the results, without providing his proofs. Fermat's\nLittle Theorem was stated in a letter he wrote in 1640. The first\npublished proof was given by Euler in 1736 (and an earlier, identical\nproof was discovered in the unpublished manuscripts of Leibniz). The\nmost famous of Fermat's results -- known as Fermat's Last Theorem --\nwas jotted down in 1637 in his copy of the book *Arithmetic* (by the\nthird-century Greek mathematician Diophantus) with the remark \"I have\ndiscovered a truly remarkable proof, but this margin is too small to\ncontain it.\" Finding a proof of Fermat's Last Theorem became one of\nthe most famous challenges in number theory. A complete solution was\nfinally given in 1995 by Andrew Wiles of Princeton University.>>\n\n**Fermat's Little Theorem:** If \\\\(n\\\\) is a prime number and \\\\(a\\\\)\nis any positive integer less than \\\\(n\\\\), then \\\\(a\\\\) raised to the\n\\\\(n\\\\)th power is congruent to \\\\(a\\\\) modulo \\\\(n\\\\).\n\n(Two numbers are said to be *congruent modulo* \\\\(n\\\\) if they both\nhave the same remainder when divided by \\\\(n\\\\). The remainder of a\nnumber \\\\(a\\\\) when divided by \\\\(n\\\\) is also referred to as the\n*remainder of a modulo n* or simply as *a modulo n*.)\n\nIf \\\\(n\\\\) is not prime, then, in general, most of the numbers \\\\(a<n\\\\) will not satisfy the above relation. This leads to the following algorithm for testing primality: Given a number \\\\(n\\\\), pick a random number \\\\(a<n\\\\) and compute the remainder of \\\\(a^n\\\\) modulo \\\\(n\\\\). If the result is not equal to \\\\(a\\\\), then \\\\(n\\\\) is certainly not prime. If it is \\\\(a\\\\), then chances are good that \\\\(n\\\\) is prime. Now pick another random number \\\\(a\\\\) and test it with the same method. If it also satisfies the equation, then we can be even more confident that \\\\(n\\\\) is prime. By trying more and more values of \\\\(a\\\\), we can increase our confidence in the result. This algorithm is known as the Fermat test.\n\nTo implement the Fermat test, we need a procedure that computes the exponential of a number modulo another number:\n\n```clj\n(defn expmod [base exp m]\n  (cond (= exp 0) 1\n        (even? exp) (mod (square (expmod base (/ exp 2))) m)\n        :else (remainder (* base (expmod base (- exp 1))) m)))\n```\n\n> Note: This is not tail recursive!\n\nThis is very similar to the `fast-expt` procedure of [section\n1.2.4](#!/sicp/ch/1/sect/2/sub/4/). It uses successive squaring, so\nthat the number of steps grows logarithmically with the exponent.<<The\nreduction steps in the cases where the exponent \\\\(e\\\\) is greater than\n1 are based on the fact that, for any integers \\\\(x, y\\\\), and \\\\(m\\\\),\nwe can find the remainder of \\\\(x\\\\) times \\\\(y\\\\) modulo \\\\(m\\\\) by\ncomputing separately the remainders of \\\\(x\\\\) modulo \\\\(m\\\\) and\n\\\\(y\\\\) modulo \\\\(m\\\\), multiplying these, and then taking the\nremainder of the result modulo \\\\(m\\\\). For instance, in the case where\n\\\\(e\\\\) is even, we compute \\\\(b^{e/2}\\pmod m\\\\), square this, and take\nthe remainder modulo m. This technique is useful because it means we\ncan perform our computation without ever having to deal with numbers\nmuch larger than m. (Compare exercise 1.25.)>>\n\nThe Fermat test is performed by choosing at randum a number \\\\(a\\\\)\nbetween 1 and \\\\(n-1\\\\) inclusive and checking whether the remainder\nmodulo \\\\(n\\\\) of the \\\\(n\\\\)th power of \\\\(a\\\\) is equal to \\\\(a\\\\).\nThe random number \\\\(a\\\\) is chosen using the procedure `rand-int`\nwhich returns a nonnegative integer less than its integer input. Hence\nto obtain a random number between 1 and \\\\(n-1\\\\), we call `rand-int`\nwith an input of \\\\(n-1\\\\) and add 1 to the result\n\n```clj\n(defn fermat-test [n]\n  (= (expmod (+ 1 (rand-int (- n 1)))\n             n\n             n)\n     a)\n```\n\nThe following procedure runs the test a given number of times, as specified by a parameter. Its value is true if the test succeeds every time, and false otherwise.\n\n```clj\n(defn fast-prime? [n times]\n  (loop [times-left times]\n    (cond (= times-left 0) true\n          (fermat-test n) (recur (- times-left 1))\n          :else false)))\n```")) "" (data/ssub {:title "Probabilistic Methods"} (md {} "Ther Fermat test differs in character from most familiar algorithms, in\nwhich one computes an answer that is guaranteed to be correct. Here,\nthe answer obtained is only probably correct. More precisely, if\n\\\\(n\\\\) ever fails the Fermat test, we can be certain that \\\\(n\\\\) is\nnot prime. But the fact that \\\\(n\\\\) passes the test, while an\nextremely strong indication, is still not a guarantee that \\\\(n\\\\) is\nprime. What we would like to say is that for any number \\\\(n\\\\), if we\nperform the test enough times and find that \\\\(n\\\\) always passes the\ntest, then the probability of error in our primality test can be made\nas small as we like.\n\nUnfortunately, this assertion is not quite correct. There do exist\nnumbers that fool the Fermat test: numbers \\\\(n\\\\) that are note prime\nand yet have th property that \\\\(a^n\\\\) is congruent to \\\\(a\\\\) modulo\n\\\\(n\\\\) for all integers \\\\(a< n\\\\). Such numbers are extremely rare,\nso the Fermat test is quite reliable in practice.<<Numbers that fool\nthe Fermat test are called *Carmichael numbers*, and little is known\nabout them other than that they are extremely rare. There are 255\nCarmichael numbers below 100,000,000. The smallest few are 561, 1105,\n1729, 2465, 2821, and 6601. In testing primality of very large numbers\nchosen at random, the chance of stumbling upon a value that fools the\nFermat test is less than the chance that cosmic radiation will cause\nthe computer to make an error in carrying out a \"correct\" algorithm.\nConsidering an algorithm to be inadequate for the first reason but not\nfor the second illustrates the difference between mathematics and\nengineering.>> There are variations of the Fermat test that cannot be\nfooled. In these tests, as with the Fermat method, one tests the\nprimality of an integer \\\\(n\\\\) by choosing a random integer \\\\(a< n\\\\)\nand checking some condition that depends upon \\\\(n\\\\) and \\\\(a\\\\). (See\n[Exercise 1.28](#!/sicp/ch/1/exercise/28/) for an example of such a\ntest.) On the other hand, in contrast to the Fermat test, one can prove\nthat, for any \\\\(n\\\\), the condition does not hold for most of the\nintegers \\\\(a<n\\\\) unless \\\\(n\\\\) is prime. Thus, if \\\\(n\\\\) passes the\ntest for some random choice of \\\\(a\\\\), the chances are better than\neven that \\\\(n\\\\) is prime. If \\\\(n\\\\) passes the test for two random\nchoices of \\\\(a\\\\), the chances are better than 3 out of 4 that \\\\(n\\\\)\nis prime. By running the test with more and more randomly chosen values\nof \\\\(a\\\\) we can make the probability of error as small as we like.\n\nThe existence of tests for which one can prove that the chance of error\nbecomes arbitrarily small has sparked interest in algorithms of this\ntype, which have come to be known as *probabilistic algorithms*. There\nis a great deal of research activity in this area and probabilistic\nalgorithms have been ruitfully applied to many fields.<<One of the most\nstriking applications of probabilistic prime testing has been to the\nfield of cryptography. Although it is now computationally infeasible to\nfactor an arbitrary 200-digit number, the primality of such a number\ncan be checked in a few seconds with the Fermat test. This fact forms\nthe basis of a technique for constructing \"unbreakable codes\"\nsuggested by Rivest, Shamir, and Adleman (1977). The resulting *RSA\nalgorithm* has become a widely used technique for enhancing the security\nof electronic communications. Because of this and related developments,\nthe study of prime numbers, once considered the epitome of a topic in\n\"pure\" mathematics to be studied only for its own sake, now turns out\nto have important practical applications to cryptography, electronic\nfunds transfer, and information retrieval.>>")) "" (data/exercises {:title "21-28"} (data/exercise {:ch 1, :number 21} (md {} "Use the `smallest-divisor` procedure to find the smallest divisor of\neach of the following numbers: 199, 1999,19999.") "" (data/q-a {} (md {} "199 is prime.\n\n1999 is also prime.\n\n7 is the smallest divisor of 19999"))) "" (data/exercise {:ch 1, :number 22} (md {} "Most Lisp implementations include a primitive (called `runtime` in\nScheme) that returns an integer that specifies the amount of time the\nsystem has been running (measured, for example, in microseconds). In\nClojure, however, there is a function that is better suited for our\ncurrent needs called `time` that evaluates its argument and prints\nthe time it took to evaluate it, then it returns the value of the\nexpression. The following `timed-prime-test` procedure, when called\nwith an integer \\\\(n\\\\), prints \\\\(n\\\\) and checks to see if \\\\(n\\\\)\nis prime while timing the test, prints the time it took to test the\nprimality of the number followed by the value of that test (ie `true`\nor `false`). But in order to successfully accomplish this task, we\nneed to learn about a few more tools in the Clojure toolshed.\n\nIf you have any questions about the specific functions used in this\n(or any example), remember to check out\n[Clojuredocs.org](http://www.clojuredocs.org).\n\n```clj\n(defn timed-prime-test [n]\n  (do (println n) (time (prime? n))))\n```\n\n> Alternatively, if we didn't want to see the `true`/`false` at the\nend, we could put `(do (time (prime? n)) n)`, which would evaluate\nthe `(time (prime? n))` procedure, which would print the time it took\nto determine the primality of \\\\(n\\\\), then it would evaluate `n`,\nand return the value, which is `n` itself.\n\nUsing this procedure, write a procedure `search-for-primes` that\nchecks for the primality of consecutive odd integers in a specific\nrange. Use your procedure to find the three smallest primes larger\nthan 10,000,000,000; larger than 100,000,000,000; larger than\n1,000,000,000,000; larger than 10,000,000,000,000. Note the time\nneeded to test each prime. Since the testing algorithm has order of\ngrowth of \\\\(\\Theta(\\sqrt n)\\\\), you should expect that the testing\nfor primes around 10,000 should take about \\\\(\\sqrt 10\\\\) times as\nlong as testing for primes around 1,000. Do your timing data bear\nthis out? How well do the data for 100,000 and 1,000,000 support the\n\\\\(\\sqrt n\\\\) prediction? Is your result compatible with the notion\nthat programs on your machine run in the time proportional to the\nnumber of steps required for the computation?") "" (data/q-a {} (md {} "```clj\n(defn search-for-primes [starting-number number-of-primes]\n  (loop [s starting-number\n         n number-of-primes]\n    (cond (= n 0) nil\n          (prime? s) (do (timed-prime-test s) (recur (inc s) (dec n)))\n          :else (recur (inc s) n))))\n```\n\nAnd I leave the observations to you."))) "" (data/exercise {:ch 1, :number 23} (md {} "The `smallest-divisor` procedure shown at the start of this section\ndoes lots of needless testing: After it checks to see if the number\nis divisible by 2 there is no point in checking to see if it is\ndivisible by any larger even numbers. This suggests that the values\nused for `test-divisor` should not be 2,3,4,5,6,..., but rather,\n2,3,5,7,9,.... To implement this chance, define a procedure `next`\nthat returns 3 if its input is equal to 2 and otherwise returns its\ninput plus 2. Modify the `smallest divisor` procedure to use `(next\ntest-divisor)` instead of `(+ test-divisor 1)` or `(inc\ntest-divisor)`. With `timed-prime-test` incorporating this modified\nversion of `smallest-divisor`, run the test for each of the 12 primes\nfound in [exercise 1.22](#!/sicp/ch/1/ex/22/). Since this\nmodification halves the number of test steps, you should expect it to\nrun about twice as fast. Is this expectation confirmed? If not, what\nis the ovserved ration of the speeds of the two algorithms, and how\ndo you explain the fact that it is different from 2?  ") "" (data/q-a {} (md {} "```clj\n(defn next [n]\n  (if (divides? 2 n)\n      (inc n)\n      (+ n 2)))\n(defn search-for-primes [starting-number number-of-primes]\n  (loop [s starting-number\n         n number-of-primes]\n    (cond (= n 0) nil\n          (prime? s) (do (timed-prime-test s) (recur (next s) (dec n)))\n          :else (recur (next s) n))))\n```\n\nWell, we're not exactly halving the number of steps because each\ntime we have to evaluate the `if` statement which includes some\ndivision and whatnot. Also there are compiler optimizations\ninvloved, so these things are rough estimates."))) "" (data/exercise {:ch 1, :number 24} (md {} "Modify the `timed-prime-test` procedure of [exercise\n1.22](#!/sicp/ch/1/ex/24/) to use `fast-prime?` (the Fermat method),\nand test each of the 12 primes you found in that exercise. Since the\nFermat test has \\\\(\\Theta(\\log n)\\\\) growth, how would you expect the\ntime to test primes near 10,000,000,000,000 to compare with the time\nneeded to test primes near 10,000,000,000? Do your data bear this\nout? Can you explain any discrepancy you find?") "" (data/q-a {} (md {} "```clj\n(defn timed-prime-test [n]\n  (do (println n) (time (fast-prime? n))))\n```"))) "" (data/exercise {:ch 1, :number 25} (md {} "Alyssa P. Hacker complains that we went to a lot of extra work in\nwriting `expmod`. After all, she says, since we already know how to\ncompute exponentials, we could have simply written\n\n```clj\n(defn expmod [base exp m]\n  (mod (fast-expt base exp) m))\n```\n\nIs she correct? Would this procedure serve as well for our fast prime\ntester? Explain.") "" (data/q-a {} (md {} "Multiplying larger numbers takes longer than the time you save by\nonly computing `mod` once. So if the numbers you're dealing with\nare sufficiently small, she's correct. But if we're trying to use\nthis for our primality test, raising 10,000,000 to the\n100,000,000,000,000 would be far more computationally expensive\nthan modding out each time."))) "" (data/exercise {:ch 1, :number 26} (md {} "Louis Reasoner is having great difficulty doing [exercise\n1.24](#!/sicp/ch/1/ex/24/). His `fast-prime?` test seems to run more\nslowly than his `prime?` test. Louis calls his friend Eva Lu Ator\nover to help. When they examine Louis's code, they find that he has\nrewritten the `expmod` procedure to use an explicit multiplication,\nrather than calling `square`:\n\n```clj\n(defn expmod [base exp m]\n  (cond (= exp 0) 1\n        (even? exp) (mod (* (expmod base (/ exp 2) m)\n                            (expmod base (/ exp 2) m))\n                         m)\n        :else (mod (* base (expmod base (- exp 1) m)) m)))\n```\n\n\"I don't see what difference that could make,\" says Louis. \"I do.\"\nsays Eva. \"By writing the procedure like that, you have transformed\nthe \\\\(\\Theta(\\log n)\\\\) process into a \\\\(\\Theta(n)\\\\) process.\"\nExplain.") "" (data/q-a {} (md {} "Remember the applicative order evaluation model. Every time the\nexponent is even, `(expmod base (/ exp 2) m)` is computed twice,\ncausing two branches of the same computation to occur."))) "" (data/exercise {:ch 1, :number 27} (md {} "Demonstrate that these Carmichael numbers: 561, 1105, 1729, 2465,\n2821, 6601, really do fool the Fermat test. That is, write a\nprocedure that takes an integer \\\\(n\\\\) and tests whether an is\ncongruent to \\\\(a\\\\) modulo \\\\(n\\\\) for every \\\\(a<n\\\\), and try your\nprocedure on the given Carmichael numbers.") "" (data/q-a {} (md {} "```clj\n(defn carmichael-check [n]\n  (loop [a 2]\n    (cond (= a n) true\n          (= (expmod a n n) a) (recur (inc a))\n          :else false)))\n```"))) (data/exercise {:ch 1, :number 28} (md {} "One variant of the Fermat test that cannot be fooled is called the\n*Miller-Rabin* test (Miller 1976; Rabin 1980). This starts from an\nalternate form of Fermat's Little Theorem, which states that if\n\\\\(n\\\\) is a prime number and \\\\(a\\\\) is any positive integer less\nthan \\\\(n\\\\), then \\\\(a^{n-1}\\equiv 1\\pmod n\\\\). To test the\nprimality of a number \\\\(n\\\\) by the Miller-Rabin test, we pick a random\nnumber \\\\(a<n\\\\) and raise \\\\(a\\\\) to the \\\\((n - 1)\\\\)st power\nmodulo \\\\(n\\\\) using the `expmod` procedure. However, whenever we\nperform the squaring step in `expmod`, we check to see if we have\ndiscovered a \"nontrivial square root of 1 modulo \\\\(n\\\\),\" that is, a\nnumber not equal to 1 or \\\\(n - 1\\\\) whose square is equal to 1\nmodulo \\\\(n\\\\). It is possible to prove that if such a nontrivial\nsquare root of 1 exists, then \\\\(n\\\\) is not prime. It is also\npossible to prove that if \\\\(n\\\\) is an odd number that is not prime,\nthen, for at least half the numbers \\\\(a<n\\\\), computing\n\\\\(a^{n-1}\\\\) in this way will reveal a nontrivial square root of 1\nmodulo \\\\(n\\\\). (This is why the Miller-Rabin test cannot be fooled.)\nModify the `expmod` procedure to signal if it discovers a nontrivial\nsquare root of 1, and use this to implement the Miller-Rabin test\nwith a procedure analogous to `fermat-test`. Check your procedure by\ntesting various known primes and non-primes. Hint: One convenient way\nto make `expmod` signal is to have it return 0.") "" (data/q-a {} (md {} "```clj\n(defn miller-rabin-prime? [n]\n  (loop [a 2]\n    (cond (= a (- n 1))            true\n          (= (mod (square a) n) 1) false\n          :else                    (recur (inc a)))))\n```"))))))
