I"î<h1 id="background">Background</h1>

<p>If youâ€™ve ever read any papers referring to quantile regression (QR), you undoubtedly have read â€œMSE yields means, MAE yields medians, and QR yields quantilesâ€. But what they hell does that even mean? Like not just hand-wavy bullshit, but really, what does that mean? After all, these are just loss functions and therefore donâ€™t <em>yield</em> anything at all. Even the regressions yields coefficients, or, more generally, models; so whence cometh the mean, median talk? It turns out theyâ€™re talking about the distribution of residuals. In this post, weâ€™ll show what they mean, how thatâ€™s the case, and weâ€™ll investigate a bit more about QR.</p>

<!-- more -->

<h2 id="what-is-it">What is it?</h2>

<p>Given a vector of residuals $r\in\mathbb{R}^n$, and a scaler $q\in[0,1]$, we define the <strong>Quantile Loss</strong> with quantile $q$ ($L_q$) as:</p>

<p>[\begin{align<em>}
L_q : \mathbb{R}^n&amp; \to \mathbb{R} <br />
L_q(r) =&amp; \begin{cases} qr &amp; r &gt; 0 \ (1-q)(-r) &amp; r \leq 0\end{cases} <br />
=&amp; \begin{cases} q|r| &amp; r &gt; 0 \ (1-q)|r| &amp; r \leq 0\end{cases}
\end{align</em>}]</p>

<h3 id="in-words">In words</h3>

<p>Basically, the loss is very much like MAE (A.K.A. $L_1$ loss) but we weight the absolute errors based on whether they are positive or negative.</p>

<h2 id="relation-to-quantiles">Relation to Quantiles</h2>

<p>A natural question to ask at this point is, when are you getting to your point? Er, I mean, what does this have to do with quantiles?</p>

<p>To answer this, letâ€™s investigate, when will the derivative equal zero (a necessary condition for a minima). To make our lives easier, let $\omega_0$ be the set of residuals that are greater than zero, and $\omega_1$ be those that are less than or equal to zero.</p>

<p>[\begin{align<em>}
0 =&amp; \frac{\partial L}{\partial r}(r) \<br />
=&amp; \sum_{r\in\omega_0}q + \sum_{r \in\omega_1}(1-q) \<br />
=&amp; q\sum_{r\in\omega_0}1 + (1-q)\sum_{r \in\omega_1}1 \<br />
=&amp; q|\omega_0| + (1-q)|\omega_1| \iff \<br />
\omega_1 =&amp; q(\omega_0 - \omega_1) \iff \<br />
q =&amp; \frac{\omega_1}{\omega_0 - \omega_1}
\end{align</em>}]</p>

<p>So we can see that the Quantile Regression optimization problem attempts to over-estimate $q$ percent of the observations. In particular, if $q = 0.5$, then the QR loss attempts to ensure that there are just as many over-estimations as under-estimations.</p>

<p>But if $q=0.9$, for instance, then this loss will attempt to ensure your model under-estimates about 90\% of the observations and over-estimates about 10\% of them.</p>

<h3 id="practice">Practice</h3>

<p>In practice it seems to converge somewhere around the quantile, but depending on the outlier situation you have, it might be somewhat off. That might be an effect of the number of iterations Iâ€™ve allowed, but either way, itâ€™s not far enough from the designated quantile to raise any alarms; especially not since whatever youâ€™re doing, itâ€™s probably using Stochastic Gradient Descent anyway.</p>

<h2 id="further-investigation">Further Investigation</h2>

<p>I lied. Iâ€™m tired and I donâ€™t think Iâ€™ll be following through on my desires for further investigation. â€œWhy donâ€™t you just delete the part where you said youâ€™d have further investigationâ€? Well, frankly, I donâ€™t think youâ€™re reading this anyway. Email me if you actually read this. Iâ€™d be super interested to find out.</p>

<h2 id="summary">Summary</h2>

<p>Quantile regression is\dots interesting.</p>
:ET