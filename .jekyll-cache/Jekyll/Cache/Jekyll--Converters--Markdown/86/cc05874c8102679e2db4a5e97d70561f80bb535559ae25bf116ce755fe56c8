I"…<h1 id="background">Background</h1>

<p>If you‚Äôve ever read any papers referring to quantile regression (QR), you undoubtedly have read ‚ÄúMSE yields means, MAE yields medians, and QR yields quantiles‚Äù. But what they hell does that even mean? Like not just hand-wavy bullshit, but really, what does that mean? After all, these are just loss functions and therefore don‚Äôt <em>yield</em> anything at all. Even the regressions yields coefficients, or, more generally, models; so whence cometh the mean, median talk? It turns out they‚Äôre talking about the distribution of residuals. In this post, we‚Äôll show what they mean, how that‚Äôs the case, and we‚Äôll investigate a bit more about QR.</p>

<!-- more -->

<h2 id="what-is-it">What is it?</h2>

<p>Given a vector of residuals $r\in\mathbb{R}^n$, and a scaler $q\in[0,1]$, we define the <strong>Quantile Loss</strong> with quantile $q$ ($L_q$) as:</p>

\[\begin{align*}
L_q : \mathbb{R}^n&amp; \to \mathbb{R} \\
L_q(r) =&amp; \begin{cases} qr &amp; r &gt; 0 \\ (1-q)(-r) &amp; r \leq 0\end{cases} \\
=&amp; \begin{cases} q\|r\| &amp; r &gt; 0 \\ (1-q)\|r\| &amp; r \leq 0\end{cases}
\end{align*}\]

<h3 id="in-words">In words</h3>

<p>Basically, the loss is very much like MAE (A.K.A. $L_1$ loss) but we weight the absolute errors based on whether they are positive or negative.</p>

<h2 id="relation-to-quantiles">Relation to Quantiles</h2>

<p>A natural question to ask at this point is, when are you getting to your point? Er, I mean, what does this have to do with quantiles?</p>

<p>To answer this, let‚Äôs investigate, when will the derivative equal zero (a necessary condition for a minima). To make our lives easier, let $\omega_0$ be the set of residuals that are greater than zero, and $\omega_1$ be those that are less than or equal to zero.</p>

\[\begin{align*}
0 =&amp; \frac{\partial L}{\partial r}(r) \\\\
=&amp; \sum_{r\in\omega_0}q + \sum_{r \in\omega_1}(1-q) \\\\
=&amp; q\sum_{r\in\omega_0}1 + (1-q)\sum_{r \in\omega_1}1 \\\\
=&amp; q\|\omega_0\| + (1-q)\|\omega_1\| \iff \\\\
\omega_1 =&amp; q(\omega_0 - \omega_1) \iff \\\\
q =&amp; \frac{\omega_1}{\omega_0 - \omega_1}
\end{align*}\]

<p>So we can see that the Quantile Regression optimization problem attempts to over-estimate $q$ percent of the observations. In particular, if $q = 0.5$, then the QR loss attempts to ensure that there are just as many over-estimations as under-estimations.</p>

<p>But if $q=0.9$, for instance, then this loss will attempt to ensure your model under-estimates about 90\% of the observations and over-estimates about 10\% of them.</p>

<h3 id="practice">Practice</h3>

<p>In practice it seems to converge somewhere around the quantile, but depending on the outlier situation you have, it might be somewhat off. That might be an effect of the number of iterations I‚Äôve allowed, but either way, it‚Äôs not far enough from the designated quantile to raise any alarms; especially not since whatever you‚Äôre doing, it‚Äôs probably using Stochastic Gradient Descent anyway.</p>

<h2 id="further-investigation">Further Investigation</h2>

<p>I lied. I‚Äôm tired and I don‚Äôt think I‚Äôll be following through on my desires for further investigation. ‚ÄúWhy don‚Äôt you just delete the part where you said you‚Äôd have further investigation‚Äù? Well, frankly, I don‚Äôt think you‚Äôre reading this anyway. Email me if you actually read this. I‚Äôd be super interested to find out.</p>

<h2 id="summary">Summary</h2>

<p>Quantile regression is\dots interesting.</p>
:ET