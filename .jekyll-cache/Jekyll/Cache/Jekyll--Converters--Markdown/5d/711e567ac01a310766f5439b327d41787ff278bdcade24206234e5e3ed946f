I"±%<h3 id="the-basics">The basics</h3>

<p>Yeah. It‚Äôs not a good sign if I‚Äôm starting out already repeating myself. But
that‚Äôs how things seem to be with linear regression, so I guess it‚Äôs fitting.
It seems like every day one of my professors will talk about linear regression,
and it‚Äôs not due to laziness or lack of coordination. Indeed, it‚Äôs an
intentional part of the curriculum here at New College of Florida because of
how ubiquitous linear regression is. Not only is it an extremely simple yet
expressive formulation, it‚Äôs also the theoretical basis of a whole slew of
other tactics. Let‚Äôs just get right into it, shall we?</p>

<!-- more -->

<h3 id="linear-regression">Linear Regression:</h3>

<p>Let‚Äôs say you have some data from the real world (and hence riddled with
real-world error). A basic example for us to start with is this one:</p>

<p><img src="/assets/images/2017/02/21-linear-regression_scatterPoints.png" alt="Data suitable for Linear Regression" title="Some Example Data" /></p>

<p>There‚Äôs clearly a linear trend there, but how do we pick which linear trend would be the best? Well, one thing we could do is pick the line that has the least amount of error from the prediction to the actual data-point. To do that, we have to say what we mean by ‚Äúleast amount of error‚Äù. For this post, we‚Äôll calculate that error by squaring the difference between the predicted value and the actual value for every point in our data set, then averaging those values. This standard is called the Mean-Squared-Error (MSE). We can write the MSE as:</p>

<p>$$\frac{1}{N}\sum\limits_{i=1}^N\left(\hat Y_i - Y_i\right)^2$$</p>

<p>where \( \hat Y_i\) is our predicted value of \(Y_i\) for a give \(X_i\). Being as how we want a linear model (for simplicity and extensibility), we can write the above equation as,</p>

<p>$$ \sum\limits_{i=1}^N\left(\alpha + \beta X_i - Y_i\right)^2 $$</p>

<p>for some \( \alpha, \beta \) that we don‚Äôt yet know. But since we want to minimize that error, we can take some derivatives and solve for \( \alpha, \beta \)! Let‚Äôs go ahead and do that! We want to minimize</p>

<p>$$ \sum\limits_{i=1}^N\left(\alpha + \beta X_i - Y_i\right)^2 $$</p>

<p>We can start by finding the \( \hat\alpha \) such that, \( \frac{d}{d\alpha}\sum\limits_{i=1}^N\left(\alpha + \beta X_i - Y_i\right)^2 = 0 \). And as long as we don‚Äôt forget the chain rule, we‚Äôll be alright‚Ä¶</p>

\[\begin{align*}
\sum\limits_{i=1}^N2\left(\alpha + \beta X_i - Y_i\right) =&amp; 0\Longrightarrow\\
\sum\limits_{i=1}^N\left(\alpha + \beta X_i - Y_i\right) =&amp; 0 \Longrightarrow\\
N\alpha + N\beta\bar X - N\bar Y =&amp; 0\Longrightarrow\\
\alpha =&amp; \bar Y - \beta\bar X \end{align*}\]

<p>and we‚Äôll find the \( \beta \) such that \( \frac{d}{d\beta}\sum\limits_{i=1}^N\left(\alpha + \beta X_i - Y_i\right)^2 = 0 \)</p>

<p>And following a similar pattern we find (sorry for the editing‚Ä¶ Wordpress.com isn‚Äôt the greatest therein):</p>

\[\begin{align*}
\sum\limits_{i=1}^N2X_i\left(\alpha + \beta X_i - Y_i\right) =&amp; 0\Longrightarrow\\
\alpha\sum\limits_{i=1}^NX_i + \beta\sum\limits_{i=1}^N X_i^2 - \sum\limits_{i=1}^NY_iX_i =&amp; 0\Longrightarrow\\
(\bar Y -\beta\bar X)N\bar X + \beta\sum\limits_{i=1}^NX_i^2 - \sum\limits_{i=1}^NY_iX_i =&amp; 0\\
N\bar Y\bar X -N\beta(\bar X)^2 + \beta\sum\limits_{i=1}^NX_i^2 - \sum\limits_{i=1}^NY_iX_i =&amp; 0\\
\beta\left(\sum\limits_{i=1}^NX_i^2 - N(\bar X)^2\right) =&amp; \sum\limits_{i=1}^NY_iX_i - N\bar Y\bar X\\
\beta =&amp; \frac{\sum\limits_{i=1}^NY_iX_i - N\bar Y\bar X}{\sum\limits_{i=1}^NX_i^2 - N(\bar X)^2}
\end{align*}\]

<p>But note:</p>

<p>$$\text{VAR}(X) = \frac{1}{N}\sum\limits_{i=1}^NX_i^2 - (\bar X)^2$$</p>

<p>So,</p>

<p>$$ N\cdot\text{VAR}(X) = \sum\limits_{i=1}^NX_i^2 - N(\bar X)^2 $$</p>

<p>And:
\(\begin{align*}
\sum\limits_{i=1}^NY_iX_i - N\bar Y\bar X =&amp; \sum\limits_{i=1}^NY_iX_i - N(\frac{1}{N}\sum\limits_{i=1}^NY_i)\bar X \\
=&amp; \sum\limits_{i=1}^NY_iX_i - \sum\limits_{i=1}^N(Y_i\bar X)\\
=&amp; \sum\limits_{i=1}^NY_i\left(X_i - \bar X\right) \\
=&amp; \sum\limits_{i=1}^NY_i\left(X_i - \bar X\right) - \sum\limits_{i=1}^N\bar Y\left(X_i - \bar X\right) + \sum\limits_{i=1}^N\bar Y\left(X_i - \bar X\right)\\
=&amp; \sum\limits_{i=1}^N\left(Y_i-\bar Y\right)\left(X_i - \bar X\right) + \bar Y\sum\limits_{i=1}^N\left(X_i - \bar X\right)\\
=&amp; \sum\limits_{i=1}^N\left(Y_i-\bar Y\right)\left(X_i - \bar X\right) = N\cdot \text{COV}(X, Y) \end{align*}\)</p>

<p>So,</p>

<p>$$\beta = \frac{\text{COV}(X,Y)}{\text{VAR}(X)}$$</p>

<p>And then we can find \( \alpha \) by substituting in our approximation of \( \beta \). Using those coefficients, we can plot the line below, and as you can see, it really is a good approximation.</p>

<p><img src="/assets/images/2017/02/21-linear-regression_scatterPoints_withLine.png" alt="Data With Linear Regression Line" title="Some Example Data With Regression Line" /></p>

<h3 id="now-we-have-it">Now we have it</h3>

<p>Okay, so now we have our line of ‚Äúbest fit‚Äù, but what does it mean? Well, it
means that this line predicts the data we gave it with the least error. That‚Äôs
really all it means. And sometimes, as we‚Äôll see later, reading too much into
that can really get you into trouble.</p>

<p>But using this model we can now predict other data outside the model.  So, for
instance, in the model pictured above, if we were to try and predict \( Y \)
when \( X=2 \), we wouldn‚Äôt do so bad by picking something around 10 for \(
Y \).</p>

<h3 id="an-example-perhaps">An example, perhaps?</h3>

<p>So I feel at this point, it‚Äôs probably best to give an example. Let‚Äôs say we‚Äôre
trying to predict stock price given the total market price. Well, in practice
this model is used to assess volatility, but that‚Äôs neither here nor there.
Right now, we‚Äôre really only interested in the model itself. But without
further ado, I present you with, the CAPM (Capital Asset Pricing Model):</p>

<p>$$ r = \alpha + \beta r_m + \epsilon $$ (where \( \epsilon \) is the error in
our predictions).</p>

<p>And you can fit this using historical data or what-have-you. There are a bunch
of downsides to fitting it with historical data though, like the fact that data
from 3 days ago really doesn‚Äôt have much to say about the future anymore. There
are plenty of cool things you can do therein, but sadly, those are out of the
scope of this post.</p>

<p>For now, we move on to</p>

<h2 id="multiple-regression">Multiple Regression</h2>

<h3 id="what-is-this-anyway">What is this anyway?</h3>

<p>Well, multiple regression is really just a new name for the same thing: how do
we fit a linear model to our data given some set of predictors and a single
response variable? The only difference is that this time our linear model
doesn‚Äôt have to be one dimensional. Let‚Äôs get right into it, shall we?</p>

<p>So let‚Äôs say you have \( k \) many predictors arranged in a vector (in other
words, our predictor is a vector in \( \mathbb{R}^n \)). Well, I wonder if a
similar formula would work‚Ä¶ Let‚Äôs figure it out‚Ä¶</p>

<p>Firstly, we need to know what a derivative is in \( \mathbb{R}^n \). Well, if
\( f:\mathbb{R}^n\to\mathbb{R}^m \) is a differentiable function, then for
any \( x \) in the domain,
\( f‚Äô(x) \) is the linear map
\( A: \mathbb{R}^n \to \mathbb{R}^m \) such that
\( \text{lim}_{h \to 0}\frac{||f(x+h) - f(x) - Ah||}{||h||} = 0 \).
Basically, \( f‚Äô(x) \) is the tangent plane.</p>

<p>So, now that we got that out of the way, let‚Äôs use it! We want to find the
linear function that minimizes the Euclidean norm of the error terms (just like
before). But note: the error term is \( \epsilon = Y - \hat Y = Y - \alpha
-\beta X \), for some vector \( \alpha \) and some matrix \( \beta \).
Now, since it‚Äôs easier and it‚Äôll give us the same answer, we‚Äôre going to
minimize the squared error term instead of just the error term (like we did in
the one dimensional version). We‚Äôre also going to make one more simplification:
That \( \alpha=0 \). We can do this safely by simply appending (or
prepending) a 1 to the rows of our data (thereby creating a constant term). So
for the following, assume we‚Äôve done that.</p>

<p>$$\begin{align} \langle\epsilon, \epsilon\rangle =&amp; (Y - X\beta)^T(Y - X\beta) \\ \langle\epsilon, \epsilon\rangle =&amp; (Y^T - \beta^TX^T)(Y - X\beta) \\ \langle\epsilon, \epsilon\rangle =&amp; Y^TY - 2Y^TX\beta + \beta^TX^TX\beta \end{align}$$</p>

<p>So, let‚Äôs find the \( \beta \) that minimizes that.</p>

<p>$$\begin{align} 0=&amp;\lim\limits_{h\to0}\frac{||(Y^TY - 2Y^TX(\beta+h) + (\beta+h)^TX^TX(\beta+h)) - (Y^TY - 2Y^TX\beta + \beta^TX^TX\beta) - Ah||}{||h||} \\ 0=&amp;\lim\limits_{h\to0}\frac{||- 2Y^TXh + 2\beta^TX^TXh + h^TX^TXh - Ah||}{||h||}\\ 0=&amp;\lim\limits_{h\to0}||- 2Y^TX + 2\beta^TX^TX + h^TX^TX - A||\frac{||h||}{||h||}\\ 0=&amp;\lim\limits_{h\to0}||- 2Y^TX + 2\beta^TX^TX - A|| \end{align}$$</p>

<p>So, now we see that the derivative is \( -2Y^TX + 2\beta^TX^TX \) and we want to find where our error is minimized, so we want to set that derivative to zero:</p>

\[\begin{align*}
	0=&amp;- 2Y^TX + 2\beta^TX^TX \\
	X^TX\beta =&amp; X^TY \\
	\beta =&amp; (X^TX)^{-1}X^TY \end{align*}\]

<p>And there we have it. That‚Äôs called the <strong>normal equation</strong> for linear regression.</p>

<p>Maybe next time I‚Äôll post about how we can find these coefficients given some data using gradient descent, or some modification thereof.</p>

<p>Till next time, I hope you enjoyed this post. Please, let me know if something could be clearer or if you have any requests.</p>
:ET