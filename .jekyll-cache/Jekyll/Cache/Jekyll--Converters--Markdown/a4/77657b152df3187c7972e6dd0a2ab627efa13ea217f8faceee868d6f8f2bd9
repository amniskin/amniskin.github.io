I"Â<h1 id="tldr">tldr</h1>

<p>The short version of this whole thing is that we can see the gradient boosting algorithm as just gradient descent on black-box models. Typically gradient descent involves parameters and a closed form solution to the gradient of your learner as a way to update those parameters. In the case of trees, for instance, there are no parameters you want to update<sup id="fnref:tree_parameters" role="doc-noteref"><a href="#fn:tree_parameters" class="footnote">1</a></sup>, and somewhat more importantly they arenâ€™t even continuous predictors, much less differentiable!</p>

<p>But gradient boosting can be seen as gradient descent on black box models resulting in an additive black box model.</p>

<h1 id="gbm">GBM</h1>

<h2 id="the-naive-algorithm">The naive algorithm</h2>

<p>A description of the algorithm Iâ€™m talking about can be found on <a href="https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm">wikipedia</a>, but Iâ€™ll go over the algorithm somewhat quickly here just to add another phrasing of the thing.</p>

<p>Letâ€™s assume we have a feature-set $X$, a response variable $y$, a loss function $L(\hat y, y)$, and a learning rate $\lambda\in\mathbb{R}_+$ such that $\lambda \leq 1$. The algorithm is as follows:</p>

<p>First we define $f_0(X) = 0$.</p>

<p>Now if $i \geq 1$, we let:</p>

<p>[\begin{align<em>}
y_i =&amp; -\frac{\partial{L(y, f_{i-1}(X))}}{\partial f_{i-1}(X)}
\end{align</em>}]</p>

<p>We then train a new set of trees (or whatever learner we want) such that:</p>

<p>[\begin{align<em>}
g_i(X) \sim&amp; y_i
\end{align</em>}]</p>

<p>We then find the real number scaler that minimizes our loss in the following equation (AKA a line-search):</p>

<p>[\begin{align<em>}
\gamma_i =&amp; \text{argmin}_\gamma(y - (f_{i-1}(X) + \gamma g_i(X)))
\end{align</em>}]</p>

<p>Finally we define:</p>

<p>[\begin{align<em>}
f_i(X) =&amp; f_{i-1}(X) + \lambda\gamma_i g_i(X)
\end{align</em>}]</p>

<p>So that</p>

<p>[\begin{align<em>}
f(X) =&amp; \sum\limits_{i=0}^N \lambda\gamma_i g_i(X)
\end{align</em>}]</p>

<p>In the special case where our loss function is mean squared error (or $L2$), our gradients are just the residuals.</p>

<h1 id="gradient-descent">Gradient Descent</h1>

<p>The typical scenario we generally talk about in gradient descent is fitting linear weights in some type of model (whether that be a linear model, logistic regression, neural network, whatever). So letâ€™s stick to this paradigm and consider the simplest case (a linear model), and weâ€™ll show how the algorithm above is actually the same algorithm, just slightly more general.</p>

<p>So weâ€™re modeling $y = X\beta + \epsilon$; in the parlance of GBM: $f = \beta$ or $f(X) = X\beta$.</p>

<p>To solve this, we define $\beta_0 = 0$.</p>

<p>Then for $i \geq 1 \in \mathbb{N}$,</p>

<p>[\begin{align<em>}
\beta_i =&amp; \beta_{i-1} - \lambda\gamma_i\frac{\partial L}{\partial \beta_{i-1}}
\end{align</em>}]</p>

<p>Where $L, \lambda, \gamma_i$ are exactly the same as in the gradient boosting algorithm<sup id="fnref:line_search" role="doc-noteref"><a href="#fn:line_search" class="footnote">2</a></sup>. So that was the standard gradient descent algorithm. Now letâ€™s show how these two are really the same beast.</p>

<h1 id="similarities">Similarities</h1>

<p>Weâ€™ll show that the update formula is the same (mutatis mutandis) in both, and then as a consequence the final models will be the constructed in the same way (because theyâ€™re both aggregates of their updates)<sup id="fnref:updates" role="doc-noteref"><a href="#fn:updates" class="footnote">3</a></sup>.</p>

<p>To make the notation easier, we define:</p>

<p>[\begin{align<em>}
\alpha_i =&amp; \frac{\partial L}{\partial \beta_{i-1}}
\end{align</em>}]</p>

<h2 id="update-formula">Update Formula</h2>

<p>For the update formula, we need to first note two things:</p>

<ol>
  <li>Gradients are linear by definition, so $X\frac{\partial L}{\partial \beta_{i-1}}$ can be seen as the result of modeling ($\frac{\partial L}{\partial \beta_{i-1}(X)}$) by the application of a linear function (although it is normally unwise to do so).</li>
</ol>

<p>[\begin{align<em>}
f_i(X) =&amp; X\beta_i \<br />
=&amp; X\left(\beta_{i-1} - \lambda\gamma_i\frac{\partial L}{\partial \beta_{i-1}}\right) \<br />
=&amp; X\beta_{i-1} - \lambda\gamma_iX\frac{\partial L}{\partial \beta_{i-1}} \<br />
=&amp; X\beta_{i-1} - \lambda\gamma_i g_i(X) \<br />
=&amp; f_{i-1}(X) - \lambda\gamma_i g_i(X)
\end{align</em>}]</p>

<p>Which is the same formula we have in gradient boosting.</p>

<h1 id="observations">Observations</h1>

<p>Since weâ€™re modeling the gradient with an arbitrary (potentially black-box) learner, we donâ€™t have the option to find the gradient with respect to the parameters, so the scale might not decrease as desired. To exemplify this, letâ€™s consider an $L_1$ objective (Mean-Absolute-Error), and a black-box learner. The gradient at each point is either 1, -1, or <code class="language-plaintext highlighter-rouge">np.nan</code> (because the absolute value function is $f(x) = \pm x$ depending on $x$). The magnitude of the gradients will never change. In a linear model we have that extra $\frac{\partial f}{\partial \beta}$ which adds scale to our gradient, but in trees we have no such thing. So in order to add scale, we tend to fit the line search and then add a learning rate to avoid over-fitting.</p>

<p>One can also sub-sample (as is a parameter in popular packages like <code class="language-plaintext highlighter-rouge">LightGBM</code>). Sub-sampling is the black-box model version of the familiar Stochastic Gradient Descent.</p>

<h1 id="summary">Summary</h1>

<p>I realize this might be obvious to some, but it was pretty cool when I first realized this. I hope you found something useful and/or interesting here.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:tree_parameters" role="doc-endnote">
      <p>Technically the split leaves in a tree define an indicator function on your data and the average value within a leaf (the prediction for that leaf) can be seen as the parameters of a tree, but this is kind of ridiculous because these are not tuned in the learning of the tree and thereâ€™s really no reason to do so (as far as I can tell).Â <a href="#fnref:tree_parameters" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:line_search" role="doc-endnote">
      <p>In practice we generally donâ€™t include the line search and just have a decreasing $\lambda$ â€“ and sometimes we donâ€™t even do that. We can get away with these shortcuts because the magnitude of the gradients will decrease as you get closer to the optima, and the derivative with respect to $\beta$ is always continuous. The same cannot be said about the gradient boosting algorithm.Â <a href="#fnref:line_search" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:updates" role="doc-endnote">
      <p>If we call our final linear model: $X\hat\beta$, then $X\hat\beta = X\left(\sum\limits_{i=0}^N\lambda\gamma_i\alpha_i\right) = \sum\limits_{i=0}^N\lambda\gamma_iX\alpha_i $. So we can see that linear regression has always been constructed as a sum.Â <a href="#fnref:updates" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET